%-------------------------------------------------------------------------------
% 
\chapter{Knowledge Acquisition Approach}
\label{chapter:approach}
%-------------------------------------------------------------------------------

This chapter defines the terms, formal structure and steps that form our 
proposed KA approach. First it introduces the general architecture and 
interaction loop that defines the sequence of interactions and steps
involved in the process(\autoref{section:Architecture}). In the second part, it
formalizes the upper ontology and logical constructs required for the KA 
approach (\autoref{section:kb}). After that, each of the crucial steps is 
described in more detail through examples and additions to the core logical 
structure defined earlier.

To make it easier to explain, formalize and understand the proposed approach 
for automated knowledge acquisition with prior knowledge, user context and 
conversational crowdsourcing, we will guide the explanations and formalizations
through an example KA conversation depicted in tables \ref{tab:conversation1}
and \ref{tab:conversation2}.

%Define step counter for chat steps
\newcounter{stepno}
\newcommand{\step}[1]{\refstepcounter{stepno}\label{#1}}

\begin{table}[h!]
\centering
\caption{Minimal example of Curious Cat and user interaction.}
\label{tab:conversation1}
\begin{tabular}{|c|l|l|}
	\hline
	\multirow{2}{2em}{Step num.} & \multicolumn{2}{c|}{Interaction}\\\cline{2-3}
	& \multicolumn{1}{c|}{Curious Cat} & User1 \\
	\hline
	\step{step:where} 1& \makecell[l]{Where are we? \\Are we at Joe's Pizza?} & Yes.\\
    \hline
	\step{step:kind} 2 & \makecell[l]{We've never been here before.\\ What kind of place this is?} & Restaurant.\\
    \hline
    \step{step:wifi} 3 & Does Joe's Pizza have Wi-Fi? & Yes. \\
    \hline
    \step{step:fast} 4 & Is it fast enough to make Skype calls? & I don't know.\\
    \hline
    \step{step:menu} 5 & What's on the menu in Joe's Pizza? & Pizzas.\\
	\hline
    \multicolumn{3}{|c|}{...Some time passes while the user eats...}\\
	\hline
	\step{step:car} 6 & What did you order? & A car. \\
	\hline
	\step{step:surecar} 7 & \makecell[l]{I've never heard of food called 'car' before. \\ Are you sure it's a type of food?} & No. \\
	\hline
	\step{step:pizzadeluxe} 8 & What did you order then? & Pizza Deluxe \\
	\hline
	\step{step:deluxething} 9 & \makecell[l]{I've never heard of 'Pizza Deluxe' before.\\What kind of thing is it?} & Pizza. \\
	\hline 
\end{tabular}
\end{table}

When the system is convesing with the user, it uses all the knowledge gathered 
in prior conversations, other user's conversations and also this conversation, 
and is able to use it to further generate new comments and related questions. 
This is evident in the interaction step number \ref{step:fast} in 
\autoref{tab:conversation1}. 
At the same time, or later, when some other user is in a similar context, 
the knowledge can be double checked with another user, as shown in 
\autoref{tab:conversation2}. Based on the votes (confirmations or rejections) 
from the crowd (other users), the system can decide whether to believe the new 
knowledge in general, or only when it interacts with this particular user.
This is explained in more detail in \autoref{section:crowdsourcing}.

\begin{table}[h!]
\centering
\caption{Minimal example of Truth checking interaction with the help of 
	crowdsourcing.}
\label{tab:conversation2}
\begin{tabular}{|c|l|l|}
	\hline
	\multirow{2}{2em}{Step num.} & \multicolumn{2}{c|}{Interaction}\\\cline{2-3}
	& \multicolumn{1}{c|}{Curious Cat} & User2 \\
	\hline
	10 & \makecell[l]{Where are we? \\Are we at Joe's Pizza restaurant?} & Yes.\\
    \hline
	11 & \makecell[l]{We've never been here before.\\ What kind of place this is?} & Restaurant.\\
    \hline
\end{tabular}
\end{table}

The resulting implementation (described at the end in 
\autoref{chapter:implementation}) of the described approach, named Curious Cat 
has a multi objective goal, KA is the primary goal, while having an intelligent 
assistant and a conversational agent are secondary goals. The aim is to perform
knowledge acquisition effortlessly and accurately as a side effect, while having
a conversation about concepts which have some connection to the user.
At the same time, the approach allows the system (or the user) to follow the 
links in the conversation to other connected topics, covering and collecting
more knowledge. For illustration see the example conversation sketch in 
\autoref{tab:conversation1}, where the topic changes from a specific restaurant
to a type of dish.

\section{Architecture}
\label{section:Architecture}
The proposed KA system consists of multiple interconnected technologies and 
functionalities which we grouped into logical modules according to
the problems they are solving (as also defined in \autoref{chapter:background}). 
This was done in order to minimize the complexity, improve the maintenance 
costs and allowing switching the implementations of separate sub-modules. 
Additionally such logical grouping increases the explainability and general
understanding of the system.

\begin{figure}[htb]
	\centering
		\includegraphics[width=0.9\textwidth]{figures/architecture.png}
	\caption{General Architecture of the KA system, with an interaction loop
			 presented as arrows.}
	\label{fig:Architecture}
\end{figure}

On \autoref{fig:Architecture} these modules are represented with the boxes,
and their functionality groups are presented with the colors (see the figure
legend). Arrows represent the interaction and workflow order and initiation 
(the interaction is initiated/triggered from the origin of the arrow).

We can see that the central core of the
system is the knowledge base (modules marked in purple and letter A). The 
knowledge base consists of \emph{Upper Ontology} gluing everything together, 
\emph{Common Sense Knowledge} to be able to "understand" user's world and check
the answers for consistency, \emph{Meta Knowledge} for enabling inference about 
its internal structures, \emph{User Context KB} to hold current user context and 
\emph{Knowledge Acquisition Rules} to drive the KA process from within the KB, 
using logical inference. 

Next to the KB, is an \emph{Inference Engine} that performs inference over 
the knowledge from the KB. Its modules are represented with the red color 
and letter B. The inference engine needs to be general enough to be able to
perform over full KB, and should be capable of meta-reasoning (over the 
meta-knowledge and KA knowledge in the KB) about the KB's internal knowledge
structures. In cases when the inference engine have some missing functionalities,
some of these tasks can be supplemented by the \emph{Procdeural Support} 
module. In the proposed system, inference engine handles almost all of the 
core KA operations, which can be separated into the following modules:
\begin{itemize}
   \item \emph{Consistency Checking} module which can asses the user's answers
   and check whether they fit within the current KB knowledge.
   \item \emph{KB Placement} module which decides where into the subtree of the
   KB the answer should be placed.
   \item \emph{Querying} module, which employs the inference engine to answer
   questions that are coming from the user through NL to Logic converter.
   \item \emph{Response Formulation} module, which employs the KA meta-knowledge
   and do inference about what to say/ask next. Results of this module are then
   forwarded to the Logic to NL converter and then to the user.
\end{itemize}

Tightly integrated with the knowledge base and inference engine is a 
\emph{Crowdsourcing Module}, which monitors crowd (multipl user's) answers and 
is able to remove (or move to different contexts) the knowledge from the KB, 
based on its consistency among multiple users. If some piece of knowledge inside
the KB is questionable, the module marks it as such and then \emph{Response
Formulation} module checks with other users whether it's true or not and should
maybe be removed or only kept in the one user's part of the KB. This module is 
represented in Green color and letter F.

At the entry and exit point of the system workflow, there are NLP processing  
modules which can convert logic into the natural language and vice versa. These
modules are used for natural language communication with the users. Tese two
modules are represented in Blue and letter E.

On the side of the Figure, there is a procedural module (depicted with Orange
color and letter D), which is a normal software module (in our implementations
written in procedural programming language), which glues everything together.
It contains a web-server, authentication functionalities, machine
learning capabilities, connections to external services and context mining
and other functions that are hard to implement using just logic and inference.
This module is taking care of the interactions between submodules. 

All of the modules are triggered either through the contextual triggers 
(also internal, like when timer detects the specific hour or time of day), or
by the users. When the context changes, it causes the system to use inference
engine to figure out what to do. Usually, as a consequence it results in a 
multiple options like questions or comments. Then it picks one and sends a 
request to the user. This triggering is represented with the arrows, where the 
blue arrows represent natural language interaction, and the orange one
represents structured or procedural interaction, when the procedural module
classifies or detects any useful change in the sensor data sent into the system
py the part running on the mobile phone.

\subsection{Interaction Loop}
\label{section:interaction}
As briefly already mentioned above, besides architecture, 
\autoref{fig:Architecture} also indicates a system/user interaction loop 
represented by arrows. Orange arrow (pointing 
directly from the phone towards the system) represents the automatic interaction
or triggers that the phone (client) is sending to the system all the time. This
provides one part of the user context. After the precedural part analyses
the data (as described in \autoref{section:minedContext} and 
\autoref{section:locationServlet}) and enter findings into the KB as 
context, this often triggers the system to come up with a new question, or 
context related info. Example of such a trigger is, when user changes a location
and the system figures out the name and type of the new place. On the other 
side, Blue arrows represent the Natural Language interaction which can happen
as a result of automatic context (Orange arrow), or some other reason causing new 
knowledge appearing in the KB. New knowledge can appear as a consequence of
answering a question from the same user, or some other user. This shows, how the
actual knowledge (even if entered automatically through procedural component) is
controlling the interaction, and explains how the system is initialized and how
its main pro-activity driver is implemented. Examples of such initialization
of the interaction is presented in \autoref{tab:conversation1} and
\autoref{tab:conversation2}. Additionally, the 
user can trigger a conversation at any point in time either by continuing the 
previous conversation or simply starting a new one. 

\begin{figure}[htb]
	\centering
		\includegraphics[width=0.9\textwidth]{figures/interactionLoop.png}
	\caption{Possible interaction types between the user and Curious Cat KA
			 System.}
	\label{fig:interaction}
\end{figure}

According to the interactions described above, the proposed KA system have two 
options for the interaction. Human to machine (HMI), when users initiate 
interaction, and machine to human (MHI), when the system initiates the 
interaction. The specifics of both, which cannot fit in 
\autoref{fig:Architecture} are explained in the following sub-sections 
\ref{section:mhi} and \ref{section:hmi}. On top of this, the design of the 
system allows a novel 
type of interaction which combines multiple users and machine into one 
conversation, while presenting this to the users as a single conversation track
with the machine. This becomes useful when the system doesn't have enough
knowledge to be able to answer user's questions, but it has just enough to know
which other users to ask (i.e. when someone is asking a question about specigic
place and there is no answer in the KB, \emph{Curious Cat} can ask other users
that it knows had been there). This type of the interaction can be called
Machine Mediated Human to Human interaction (MMHHI). This allows the system
to answer questions also when it doesn't know them, while simultaneously also
store and remember the answers, either parsing them and assert them directly to 
KB, or leave them in NL for later Knowledge Mining analysis. The possible
interaction types are also presented on \autoref{fig:interaction}.

\subsubsection{Machine to Human Interaction (MHI)}
\label{section:mhi}
The most basic form of interaction between the CC system and the user, which
we also use the most, is when something triggers a change in the KB and 
CC decides it's the time to ask or tell something. On the 
\autoref{fig:interaction}, this is represented by the most inner loop (MHI). 
Example of this is when the context part of \emph{Procedural} module classifies
a new location and then asserts it into the KB. This then triggers Inference 
Engine which results in a new user query (same as defined in Definition
\ref{def:userLocation}).

\begin{equation*}
	\label{eq:ccWantsLoc}
	ccWantsToAsk(CCUser1, (userLocation(CCUser1,CCLoc1)))
\end{equation*}

This query then goes through logic to NL (\autoref{section:logicNL}) 
conversion, which is then
presented to the user in NL like "Where are we? Are we at the restaurant X?".
This is represented on \autoref{fig:interaction} with a blue arrow on the
inner circle, marked with 1a. The presentation is handled by the client and 
can be in a written form, or through the text-to-speech interface. User can then 
answer this question and thus close the interaction loop (blue arrow marked with
1b), possibly causing a new one with her answer. 

For easier answering, the KA system can use existing KB to generate a set of 
possible answers at the qustion generation time. These can be then picked by
the user instead of writing. The guidance can consis of variations of these:
\begin{itemize}
	\item A fixed set of pre-defined options that user can pick from, generated
	from the KB.
	\item A set of pre-defined options with an additional free text field when
	the set of possible answers is big or inifite. In this cases the text field
	is connected to the KB providing auto-complete options for valid answers.
	\item Completely free text were user can write anything. This is essentially
	the same as for the HMI interaction described below (\autoref{section:hmi}).
\end{itemize}

The example of mediated answer guidance can be seen in \autoref{fig:ccmediated},
where the system presented a set of possible answers while still allowing a 
free text 
which will be autocompleted with the food types that the system knows about. 
If the user enters something new, the system will accept that (as shown 
in the step \ref{step:car} in \autoref{tab:conversation1}).

The inference triggering, language rules and mechanisms for context detection
are described in more detail in sections \ref{section:nl} and 
\ref{section:dialog} respectively. This
type of the interaction is where the users answer questions and is thus part of
the main research topic of this thesis.

\subsubsection{Human to Machine Interaction (HMI)}
\label{section:hmi}
The second type of interaction is, when user initiates the conversation. 
If this is done at some point as the answer to an old question, the process is 
the same as described above in subsection \ref{section:mhi}. But users can 
also enter a 
free text, asking a question or stating something. In this case, this goes 
through the NL to Logic conversion (section \ref{section:NLLogic} and section 
\ref{sectopn:locationServlet}) which tries to convert the
text into logical query or assertion. The complexity of converting NL into logic 
is a lot higher than in the opposite direction, since the language is not
as exact. In CC implementation (section \ref{section:locationServlet}), we 
handled this to
some extent by using SCG system \parencite{Schneider2015}, where the text
is matched to NL patterns which are linked to appropriate logical structures.
This would be used for example, when user, instead of simple "Pizza Deluxe" 
(step \ref{step:pizzadeluxe} in \autoref{tab:conversation1}, would say 
"They sell pizzas", or something even more complex (see section 
\ref{section:NLLogic}). After the text is converted into logic, inference engine 
can use it to query it against the KB, and show the answers back to user, again 
converted into NL through \emph{Logic to NL Converter} module. This type of
interaction is depicted on \autoref{fig:interaction} by the middle arrow circle
started by humans (arrow 2a), where machine provides the response (arrow 2b).

%SUBSECTION
\subsubsection{Machine Mediated Human to Human Interaction (MMHHI)}
\label{section:mmhhi}
Both of the interactions described in sections \ref{section:mhi} and 
\ref{section:hmi} pre-supposes that the receipient of the query, knows how to
answer it, or respond otherwise. In the cases when, let's say machine doesn't
have any answer (The NL question gets converted into the logical query, which doesn't retrieve any aswers from within the KB). It could respond with 
"I don't know", which is a valid response.
Thile this allows for the conversation to continue, it doesn't help the user
to get the answer, also does not benefit to knowledge acquisition. The only 
thing the system can learn from this, is that user is interested in the object
of the question. This doesn't have to end there though, since CC has access to
other users and knowledge about their past and current contexts. Based on the
topic of interes from the user query, the system can easily find users which
might know the answer (inferred from their past whereabouts, answers, etc.).
Once such an user is deteced, the original question can simply be forwarded to
him, as it would be asked by CC itself. Once he, or one of the users answers,
CC can forward it to the original user. On top of that, CC can parse the answer
the same way as described above for HMI and MHI (sections \ref{section:mhi},
\ref{section:hmi}), and remember it, placing it into the KB. In the cases when
the language of the answer is too complex, it can be stored in its original
format, for later text-mining apporach which can lead to learning of new-patterns
as well as the knowledge hiding in the answers. On top of this, CC can also
remember the question itself, and place it on specific type of concepts, as an
important quetsion to ask. On \autoref{fig:interaction},
MMHHI approach is depicted by the outer circle of arrows, where 3a is original 
user's question, which is forwarded to other users when CC doesn't know how to 
answer (3b). After one or more of the users answer, the answer is forwarded back
to the original user (3c).

%SECTION
\section{Knowledge Base}
\label{section:kb}
As visible in \autoref{fig:Architecture}, Knowledge Base is the central part
of the proposed KA system. Internally KB has three components. The main part, 
which should in any real implementation of the system also be the biggest, is 
the common-sense knowledge, and its upper ontology over which we operate. 
This part of the system contributes the most to the ability to check the 
answers for consistency. The more knowledge already exists, the easier becomes 
to assess the answers, come up with new questions and also propose possible
answers in the guided interaction(\autoref{section:existingKB}).

The second part is the user Context KB, which stores the contextual knowledge 
about the user. This covers the knowledge that the user has provided about 
himself (\autoref{section:acquiredContext}) and the knowledge obtained by 
mining raw mobile sensors (\autoref{section:minedContext}). On 
\autoref{fig:Architecture}, This part of the KB is represented 
as the left-most KB, sitting between the main KB and the 
\emph{Procedural Module}.The sensor based context allows the system to 
proactively target the right users at the right time and thus improve the 
efficiency and accuracy and also stickiness of the KA process.

The third KB part, is the meta-knowledge and KA rules that drive the dialog and 
knowledge acquisition process (section \ref{section:kakb}). Although in our 
implementation we 
used Cyc KB (\autoref{section:cyckb}), the approach is not 
fixed to any particular knowledge base. But the KB needs needs to be expressive
enough to be able to cover the intended knowledge acquisition tasks and 
meta-knowledge needed for the system's internal workings. 

Because the full Curious Cat system including the KB is too big and complex to 
be fully explained here (the KA Meta Knowledge alone consists of 12,353 
assertions and rules), we will focus on the fundamentals of the idea and 
approach, and define the simplest possible logic to explain the workings 
through the examples given in the \autoref{tab:conversation1} and 
\autoref{tab:conversation2}. 

The logic examples are given in formal higher order predicate logic, which we 
later replace with a more compact notation, with a slight change in the way the 
variables are presented. For better readability, instead of $x, y, z$, we mark 
variables with a question mark ($?$) followed by a name that represents the 
expected type of the concepts the variable represents. For example, when we see 
a variable in a logical formula like $CCUser(?PERSON)$, we immediately know that
$?PERSON$ can be only replaced with (bound to) instance or subclasses of the 
concept $Person$. We start predicate names with a small letter 
($predicate$) and the rest of the 
concepts with a capital letter ($Concept$). At this point it is worth noting 
that while our logical definitions and formalization are strongly influenced by 
Cyc\parencite{Lenat1995}, and while the approach is based on the Cyc upper 
ontology, the approach is general and not bound to any particular 
implementation, and our notation below reflects but is not tightly bound to 
that of OpenCyc\footnote{The notation here follows closely practices used in 
Cyc. For more details, readers can refer to \parencite{Lenat1995} and
\parencite{Matuszek2006}) and the references it contains.}. 

%SUBSECTION
\subsection{Upper Ontology}
\label{section:upperOnto}
First we introduce the vocabulary or terms (constants/concepts) that will allow 
us to construct the upper ontology which is the glue of any knowledge base that 
can be used for machine inference:

\begin{equation}\label{set:terms}
\begin{gathered}
S_{Constants} = \{Something, Class, Number, Predicate, subclass, \\
	is, arity, argClass, argIs\}
\end{gathered}
\end{equation}

In the standard \emph{predicate logic}, $P(x)$ notation tells us that whathever 
the $x$ stands for, it has the property defined by the predicate $P$. 
For example, the following propositional function:

\begin{equation}\label{eq:examplepred}
Person(x)
\end{equation}
is stating that something ($x$) is a person, or more precisely, $x$ is an 
instance of a class $Person$. In order to be able to construct logical 
statements ranging
over classes and their instances in a more controlled and transparent way, we
use a constant $is$, and define it as a predicate denoting that something is an
instance of some class.

\begin{definition}[predicate "$is$"]
\label{const:is}
Predicate $is(x,y)$ denotes that $x$ is an instance of $y$. For example, 
stating $is(John, Human)$ defines John as one instance of the class of humans.
\end{definition}
Now, to make things clearer and more precise, instead of writing instance
relation through custom predicate $Person(x)$, we can use more precise syntax, 
which allow us to specify what is instance of which class: $is(x,Person)$.

As visible in the \autoref{eq:examplepred}, in predicate logic, predicates are
defined only with usage. Everything that we write as a predicate in a similar
formula is then defined as a predicate. This is not a desired behavior in our 
KA approach, since the knowledge will be coming from the users with various
backgrounds and without any idea of predicate logic. For this reason we
need more control of what can be used where, if we want to be able to
check for consistencies and have control of our KB with the inference engine.
For this reason, we enforce a constraint (rule).

\begin{definition}[Predicate Constraint]\label{constraint:predicate}
Everything that we want to use in the KB as a predicate, must first be defined
as an instance of the $Predicate$ class:
$\forall x \in S_{Predicates}: is(x,Predicate)$. From the other side, set of
predicates can be defined as $S_{Predicates}=\{x:is(x,Predicate)\}$.
This constraint can be inserted into our KB as a \emph{material implication}
rule, which needs to be true at all times, to serve as a constraint:
\begin{equation}\label{rule:pred_constraint}
\forall P \forall x_{1...n} (P(x_1...x_n) \implies is(P,Predicate))
\end{equation}
\end{definition}

Now, careful reader might notice, that we actually cannot use $is$ as a 
predicate, since nowhere in our KB is stated that this is actually a 
predicate. To fix this error and make our KB consistent with its constraints,
we need to add an assertion defining what term $is$ stands for:

\begin{equation}\label{as:is_is}
is(is, Predicate)
\end{equation}
At the time the above assertion (Assertion \ref{as:is_is}) is asserted
into the KB, it also becomes valid assertion, since it complies with the
constraint defined in Definition \autoref{constraint:predicate} and thus our
Constraint Rule \ref{rule:pred_constraint} is true. After this 
assertion is in the KB, $ir$ can be used as a predicate because it is an 
instance of the term $Predicate$ and complies with our constraints. 

At this point we can define(assert) the rest of our predicates:

\begin{equation}\label{as:predicates}
\begin{gathered}
is(subclass, Predicate) \land is(arity,Predicate) \land is(argClass,Predicate)\\
\land is(argIs,Predicate)
\end {gathered}
\end{equation}
And also the rest of our terms, which we define as instances of term $Class$.
\begin{equation}\label{as:is_class}
\begin{gathered}
	is(Class,Class) \land is(Predicate,Class)\\ 
\land is(Number, Class)
\end {gathered}
\end{equation}

In Predicate logic, predicates have a property called arity, which defines 
number of arguments that the predicate can have. For example, if predicate
$P$ has arity of 1, then it can only take one operand (variable or term). In 
this case only $P(x)$ or $P(a)$ are valid statements, and $P(x,y)$ is not.
In our KB, arity is defined using $arity$ predicate, which itself was defined
in Assertion \ref{as:predicates}.

\begin{definition}[predicate "$arity$"]\label{def:arity}
Predicate $arity(x,y)$ denotes that predicate $x$ has arity of $y$.
\end{definition}

Similarly, as with the constraint that all predicates need to be defined as
such (Definition \autoref{constraint:predicate}), we gain more control over 
KB and make things easier for the inference engine and KA approach, if we 
limit the assertions, to "obey" the predicate arities. For this reason our KB
has additional constraint.

\begin{definition}[Arity Constraint]\label{constraint:arity}
All assertions in CC KB are valid only, when the predicates used in the 
assertion have the same number of arguments as defined with their $arity$ 
assertions:
\begin{equation}\label{rule:arity_constraint}
\forall P \forall n \forall x_{1...n}:(P(x_1,...,x_n) \implies arity(P,n))
\end{equation}
\end{definition}

After we add the above rule (Constraint Rule \ref{rule:arity_constraint}), our 
KB is not consistant anymore, since all the $is(x,y)$ assertions (Assertions 
\ref{as:is_is}, \ref{as:predicates}) violate the constraint. We fix this by 
adding the following assertion:

\begin{equation}\label{as:arity_is}
arity(arity,2) \land arity(is,2)
\end{equation}
This makes the KB consistant again, because we defined all the arities of
predicates $arity$ and $is$, which we have used so far in our KB, as well as 
defined them with $is(x, Predicate)$ assertions. We can now continue with 
defining the arities of the rest of the predicates:

\begin{equation}\label{as:arity_predicates}
\begin{gathered}
arity(subclass, 2) \land arity(argClass,3) \land arity(argIs,3)
\end {gathered}
\end{equation}
We can see now that the arity of $is$ predicate is defined as 2 (same as for 
$subclass$ and $arity$, which can be used to define arity of itself), and can 
confirm that all the logical formulas in the definitions up to now are correct.

To be able to describe the world in more detail, we define the $subclass$ 
predicate, which handles the hierarchy relations between multiple classes 
(unlike $is$, which handles relationships between classes and their instances).

\begin{definition}[predicate "$subclass$"]\label{def:pred_subclass}
Predicate $sublcass(x,y)$ denotes that $x$ is a subclass of $y$. For example, 
asserting $subclass(Dog, Animal)$, is specifying all dogs, to be a sub-class of
animals, and $subclass(Terrier,Dog)$ is speficfying that all terriers are 
sub-class of dogs. At this point we might notice that while it is logical to us
that terriers are also a sub-class of animals, there is no way for the machine
inference to figure that out. For this reason we need to introduce a
"subclass transitivity" inference rule:
\begin{equation}\label{rule:subclass_transitivity}
\begin{gathered}
  \forall x \forall y \forall z: ((subclass(x,y) \land subclass(y,z)) \implies subclass(x,z))
\end{gathered}
\end{equation}
The rule above is basically saying that if a first thing is a sub-class of a
the second thing, and then thesecond thing is a subclass of the thirdthing,
then the third thing is a sub-class of the first thing as well. 
\end{definition}

Because we want to be able to prevet our system from acquiring incorrect 
knowledge, we need to limit the domains and ranges of the predicates 
(arguments). This could be done by adding a speciic constraint rules
(material implication rule without the power to make new assertions). For 
example, for both $subclass$ arguments, to only allow instances of a $Class$, 
we could assert:
\begin{equation}\label{as:domain_example}
  \forall x_1 \forall x_2: (subclass(x_1,x_2) \implies (is(x_1,Class) \land 
  is(x_2,Class))
\end{equation}
Because the rule (Rule \ref{as:domain_example} is only true if the right part
(the consequent) is true, or the left part (the antecedent) is false, its
inclusion in the KB (as with other constraint rules) forces the KB to not allow
the arguments of subclass to be anything else than an instance of a class 
$Class$. It would be hard to construct a large KB, by writing the rule
like this for each of the thousands potential predicates. To make this easier,
following Cyc practice\parencite{Lenat1995}, we will introduce $argIs$ 
predicate (Definition \ref{def:pred_subclass}). To make this definition more understandable, let's
first expand the example (Rule \ref{as:domain_example} above:

\begin{equation}\label{as:domain_ext_example}
\begin{gathered}
  \forall x_1 \forall x_2: (subclass(x_1,x_2) \implies (is(x_1,Class) \land is(x_2,Class)) \\ 
  \iff \\
  argIs(subclass,1,Class) \land argIs(subclass,2,Class))
\end{gathered}
\end{equation}
This rule above (Rule \ref{as:domain_ext_example} states, that the constraint
rule (Rule \ref{as:domain_example} can be written as 2 $argIs$ assertions.
Instead of writing full rule, the constraint for the argunent of $subclass$ can
be written simply as $argIs(subclass,1,Class)$. To make this hold for all the
combinations of predicates (not just $subclass$ from example), we can
re-phrase the rule to be general, and also define the $argIs$ predicate.

\begin{definition}[predicate "$argIs$"]\label{def:pred_argis}
Predicate argIs(x,y,z) denotes that the $y-th$ argument of predicate $x$, must
be an instance of $z$. For example, asserting \\$argIs(subclass,1, Class)$, states
that the first argument of predicate $subclass$ must be an instance of $Class$.
This is enforced by the following constraint rule:
\begin{equation}\label{as:domain_isa_constraint}
\begin{gathered}
  \forall P \forall n \forall x_1...x_n \forall C_{1...m}: \\
  ((arity(P,n) \land P(x_1,...,x_n)) \implies (is(x_1,C_{1...m}) \land ... \land is(x_n,C_{1...m})) \\ 
  \iff \\
  argIs(P,1,C_{1...m}) \land ... \land argIs(P,n,C_{1...m}))
\end{gathered}
\end{equation}
\end{definition}

These definitions allow us to use simple $argIs$ assertions, instead of
complicated rules. For the cases, when the arguments shouldn't be instances
of a $Class$, but its subclasses, we can define similar predicate and its
constraint rules also fro $argClass$:

\begin{definition}[predicate "$argClass$"]\label{def:pred_argClass}
Predicate $argClass(x,y,z)$ denotes that the $y-th$ argument of predicate $x$, 
must be a subclass of $z$. For example, asserting 
\\$argClass(servesCuisine, 2, Restaurant)$, states that the first argument of 
predicate $servesCuisine$ must be a subclass of $Restaurant$.
This is enforced by the following constraint rule (similar as for $argIsa$, but
for $argClass$):
\begin{equation}\label{as:domain_class_constraint}
\begin{gathered}
  \forall P \forall n \forall x_1...x_n \forall C_{1...m}: \\
  ((arity(P,n) \land P(x_1,...,x_n)) \implies (subclass(x_1,C_{1...m}) \land ... \land subclass(x_n,C_{1...m})) \\ 
  \iff \\
  argClass(P,1,C_{1...m}) \land ... \land argClass(P,n,C_{1...m}))
\end{gathered}
\end{equation}
\end{definition}

Before we can assign argument constraints to our existing predicates and have
all the KB valid, we need to define a special class $Number$:

\begin{definition}[class $Number$ and its instances]\label{def:number}
All natural numbers are instances of the class $Number$. Formally, this can be
asserted into a KB as:
\begin{equation}\label{as:numbers}
	\forall x \in \mathbb{N}:is(x,Number)
\end{equation}
\end{definition}

This now allows us to use $argIs$ and $argClass$ predicates instead of 
complicated rules, to define types of arguments inside any predicate used in the
KB. By using these two newly defined predicates, we can now proceeed to assert 
the argument limits of the $argIsa$ predicate itself: 

\begin{equation}\label{as:argIs_domain}
\begin{gathered}
argIs(argIs,1,Predicate) \land argIs(argIs,2,Number) \land argIs(argIs,3,Class) 
\end{gathered}
\end{equation}
We can see that the first argument must be an instance of $Predicate$, which 
holds in all three cases ($argIs$ is a first argument of the Assertion 
\ref{as:argIs_domain} above), since it was defined in Assertion 
\ref{as:predicates}. Similary, the second argument is a valid number, in all 
three cases as defined in Definition \ref{def:number}. Also the third arguments
($Predicate$, $Number$,$Class$), are all instances of the $Class$ as defined in
Assertion \ref{as:is_class}, so the assertion \ref{as:argIs_domain} can be asserted
and it doesn't invalidate itself throigh argument contraint rule (Constraint 
\ref{as:domain_isa_constraint}).

Similary, we can now proceed to define the rest of our predicates. Starting with
the most similar $argClass$:
\begin{equation}\label{as:argClass_domain}
\begin{gathered}
	argIs(argClass,1,Predicate) \land argIs(argClass,2,Number) \\
	\land argIs(argClass,3,Class) 
\end{gathered}
\end{equation}
Since we didn't yet assert any direct $argClass$ constraint, this predicate
at this point defines the constraints, without any danger to invalidate our
current KB. 

Continuing with $subclass$. On the Definition \ref{def:pred_subclass}, 
we can see that this
predicate defines sub-class relationships between the classes. For this reason
it makes sense to only allow instances of $Class$ for its arguments:
\begin{equation}\label{as:subclass_is_constraint}
	argIs(subclass,1,Class) \land argIs(subclass,2,Class)
\end{equation}
Same as with the Assertion \ref{as:argClass_domain}, we didn't yet assert any 
direct assertions about something being a sub-class of something, this assertion
doesn't affect yet the validity of our KB, while prevents future assertions of 
$subclass$ predicate on anything but $Class$ instances.

For the $arity$ predicate, we can check our existing assertions 
(\ref{as:arity_is}, \ref{as:arity_predicates}), and
see that as the first argument we always have an instance of $Predicate$, while
as the second argument we have an instance of a $Number$. According to this, it
serves our purpose and is safe to limit the arguments of $arity$ to:
\begin{equation}\label{as:arity_is_constraint}
	argIs(arity,1,Predicate) \land argIs(arity,2,Number)
\end{equation}

We can now proceed to define our last ($is$) predicate constraints, which is
a bit more complicated. If we look at our existing  $is$ assertions 
(\ref{as:is_is}, \ref{as:predicates}, \ref{as:numbers}), we can
see that as a second argument we always have an instance of a $Class$ 
($Predicate$, $Class$), but
for the first argument we can actually put in anything ($is$, $Class$, 
$Predicate$,  $Number$, $\mathbb{N}$). From instance of a $Class$,
instance of a $Predicate$, to an instance of a $Number$. For the second argument
we can imediatelly asert

\begin{equation}\label{as:is_arg2_domain}
argIs(is,2, Class)
\end{equation}

In order to be able to say that some argument can be anything, we followed a 
Cyc example and introduce term $Something$, first mentioned in the set of our 
terms in Assertion \ref{set:terms}. We set this as the constraint for the first
argument of $is$ predicate:

\begin{equation}\label{as:is_arg1_domain}
argIs(is,1, Something)
\end{equation}
But this assertion above (\ref{as:is_arg1_domain}), invalidates the correctnes
of all of our $is$ assertions, since none of the current first arguments are
instances o $Something$ (see Assertions \ref{as:is_is}, \ref{as:predicates} and 
\ref{as:numbers}). To fix this, we need to be able at least to say that things 
are instances of $Something$ ($is(x,Something)$). According to the $is$ argument
constraint assertion above (\ref{as:is_arg2_domain}), $Something$ must be an 
instance of a $Class$. So we define it as so:

\begin{equation}\label{as:is_something}
	is(Something,Class)
\end{equation}
Now, as a consequence of this, we could assert for all of the arguments that
are used in $is$ ($is$, $Class$, $Predicate$, $Number$, $\mathbb{N}$), that 
they are an instances of the $Something$. This would
be highly unpractical, since we would need to do this for every future constant
to be used by $is$ predicate (especially unpractical for infinite number of
$\mathbb{N}$).

Instead, since we know that $Something$ is an instance of a $Class$, we can use 
it in our $subclass$ assertions (a consequence of constraint Assertion 
\ref{as:subclass_is_constraint}) and state that $Class$ is a subclass of 
$Something$:
\begin{equation}\label{as:subclass_something}
	subclass(Class, Something)
\end{equation}
A consequence of this assertion is (because of inference rule 
\ref{rule:subclass_transitivity}), that
every sub-class of anything that exists in our KB (because we can only use 
instances of $Class$ in $subclass$ assertions), is a sub-class of $Something$ as
well. This doesn't yet seem to help us make our 1st $is$ predicate arguments 
instances of $Something$, but it will, after we address another weaknes in our
current KB. 
Consider the continuation of the example we started in $subclass$ 
definition (Definition \ref{def:pred_subclass}, where a terrier is a sub-class
of dog, and dog is a sub-class of animals ($subclass(Dog,Animal)$, 
$subclass(Terrier,Dog)$). If we introduce an instance of the
class $Terrier$, let's say, a real dog named "Spot" ($is(Spot,Terrier$)), we can
see that there is a logical problem, since Spot is a terrier in our KB, but
not a dog, or even an animal (there is nothing to support $is(Spot,Animal)$ 
assertion. This can be fixed by introducing the following Inference Rule:

\begin{equation}\label{rule:is_transfer}
\begin{gathered}
  \forall x \forall y \forall z: ((is(x,y) \land subclass(y,z)) \implies is(x,z))
\end{gathered} \end{equation}
This rule is basically saying, that if there is an instance of a class, and this
class is a sub-class of another class, then this instance is also an instance of
the other class. Now, this inference rule (\ref{rule:is_transfer}), together 
with the fact that $Class$ is a sub-class of $Something$, and the $subclass$
transitivity inference rule (\ref{rule:subclass_transitivity}), makes everything
 that is instance of a 
class, or its sub-class (which is everything in our KB), also an instance of
$Something$, and thus proving the assertion \ref{as:is_arg1_domain} correct and 
consequently our KB fully consistant again.

At this point our Upper Ontology is defined (it is visually presented on
\autoref{fig:upperOnto}) and ready to build upon as will be
described in the next chapters. Since Curious Cat main implementation is based
on Cyc, and it was inspired by the way Cyc ontology is constructed and being 
used, this upper ontology reflects the main part of Cyc upper ontology 
(see Chapter \ref{chapter:implementation}, implementation on how this 
formalization maps to Cyc). While our upper ontology logical definitions and 
formalizations are strongly influenced by Cyc, the approach is general and not 
bound to any particular implementation, and our notation reflects but is not 
tightly bound to that of Cyc. For example, the usage of $argIs$ and $argClass$
can be replaced by $domain$ and $range$ when using a RDF schema, such as was 
done in our RDF prototype implenentation\parencite{Bradesko2012a}, or a 
completely custom constraints can be used in specific ontologies, so this
upper ontology is more of a guidence and a tool to be able to explain apporach,
than a fixed ontology that needs to be implemented.

\begin{figure}[H]
	\centering
		\includegraphics[width=0.7\textwidth]{figures/upperOntology.png}
	\caption{Upper ontology terms, with 'is' and 'subclass' relations.}
	\label{fig:upperOnto}
\end{figure}

%SUBSECTION
\subsection{Existing Knowledge}
\label{section:existingKB}
This sub-chapter extends our upper ontology example with additional knowledge
that allows us to explain the system through the examples given in 
\autoref{tab:conversation1} and \autoref{tab:conversation2}. 

As also visible in the Architecture schematic (\autoref{fig:Architecture},
KB with background knowledge is one of the most crucial elements of proposed 
approach. It serves both, as the driving force behind the source of the 
questions, since with the help of contextual knowledge triggers the infrence to
produce logical queries for the missing parts of the knowledge. At the same time
it is the drive behind the proactive user interaction, sarting either as a
question or a suggestion. Finally, the background knowledge is also used for 
validation of answered questions. If the answers are not consistent according
to the existing KB, users are required to re-formulate, or repeat the answer.

The main \emph{Curious Cat} implementation, uses an extended 
full Cyc ontology and KB, similar to that released as ResearchCyc, as a 
common sense and background knowledge base. This is far too big (millions of 
assertions), to be explained in any detail here with the general approach (it 
is explained in more detail in Chapter \ref{chapter:implementation}, 
Implementation). In this chapter we define only the concepts and predicates 
and thus construct the minimal example KB that is necessary for explaining the p
roposed system.

First we introduce the set of new concepts that we need for the food part
of the ontology. These concepts are defined on top of the existing Upper 
Ontology, so the new parts of the KB should maintain the consistency of the
already described parts. The set of terms $S_{Food} $, needed to describe this 
part of the KB is as follows:

\begin{equation}\label{set:foodTerms}
\begin{gathered}
S_{Food} = \{FoodOrDrink,Food,Bread,Baguette,Drink,Coffee\}
\end{gathered}
\end{equation}
Where each of these terms is an instance of a $Class$:
\begin{equation}\label{set:foodTermsClass}
\begin{gathered}
\forall x \in S_{Food}: isa(x,Class)
\end{gathered}
\end{equation}
Then, these classes are connected into a class-hierarchy, which is done with
a $subclass$ predicate:

\begin{equation}\label{as:kbFoodSubclasses}
\begin{gathered}
    subclass(Drink,FoodOrDrink) \land subclass(Coffee,Drink) \land \\
	subclass(Food,FoodOrDrink) \land subclass(Bread,Food) \land \\
	subclass(Baguete, Bread)
\end{gathered}
\end{equation}
Which is also presented graphically on \autoref{fig:foodSubclass} below.
\begin{figure}[H]
	\centering
		\includegraphics[width=0.6\textwidth]{figures/foodOntology.png}
	\caption{Hierarchy of food related terms, specified by subclass predicates.}
	\label{fig:foodSubclass}
\end{figure}

The second part of ontology (KB) constructing knowledge about places consist
of the following terms:

\begin{equation}\label{set:placeTerms}
\begin{gathered}
S_{Place} = \{Placa,PublicPlace,PrivatePlace,Restaurant\}
\end{gathered}
\end{equation}
Where each of these terms is also an instance of a $Class$:
\begin{equation}\label{set:placeTermsClass}
\begin{gathered}
\forall x \in S_{Place}: isa(x,Class)
\end{gathered}
\end{equation}
Then, these classes are connected into a class-hierarchy, which is done with
a $subclass$ predicate:

\begin{equation}\label{as:kbPlaceSubclasses}
\begin{gathered}
    subclass(PrivatePlace,Place) \land subclass(PublicPlace,Place) \land \\
	subclass(Restaurant,PublicPlace) \land subclass(Home,PrivatePlace)
\end{gathered}
\end{equation}
And, in order to have some "real-world" knowledge, we also add an example 
instance of a real restaurant represented as term $Restaurant1$.
\begin{equation}\label{as:restaurant1}
	is(Restaurant1,Restaurant)
\end{equation}
This is all presented graphically on \autoref{fig:placeOntology} below.
\begin{figure}[H]
	\centering
		\includegraphics[width=0.5\textwidth]{figures/placeOntology.png}
	\caption{Hierarchy of place related terms, specified by subclass predicates.}
	\label{fig:placeOntology}
\end{figure}

Then the last part (excluding predicates which are defined separately),
of our \emph{Existing Knowledge} consist of the following terms:

\begin{equation}\label{set:otherTerms}
\begin{gathered}
S_{Reest} = \{Service,WirelessService,Vehicle,Car,Animal,Duck,\\
	Human,User\}
\end{gathered}
\end{equation}
Where each of these terms is also an instance of a $Class$:
\begin{equation}\label{set:restTermsClass}
\begin{gathered}
\forall x \in S_{Rest}: isa(x,Class)
\end{gathered}
\end{equation}
And a class-hierarchy:

\begin{equation}\label{as:kbPlaceSubclasses}
\begin{gathered}
    subclass(Car,Vehicle) \land subclass(WirelessService,Service)\land\\
	subclass(Duck,Animal)\land subclass(Human,Animal)\land \\
	subclass(User,Human)
\end{gathered}
\end{equation}
And an instance of the $User$ class representing one CC user.
\begin{equation}
	is(User1,User)
\end{equation}

Which is also presented graphically on \autoref{fig:restOntology} below.
\begin{figure}[H]
	\centering
		\includegraphics[width=0.9\textwidth]{figures/restOntology.png}
	\caption{Hierarchy of the rest of the terms from our \emph{existing 
	knowledge.}}
	\label{fig:restOntology}
\end{figure}
We can see that all these new terms add on top of the existing set of terms from 
upper ontology ($S_{Constants} = S_{upper} \cup S_{Food} \cup S_{Place} 
\cup S_{Rest}$). 

For defining the predicates, we need a bit more detailed definitions, since we 
also want to represent the constraints which will serve for the inference 
engine to check the validity of the answers (as explained in the definitions
\ref{def:pred_argis} and \ref{def:pred_argClass}).

\begin{definition}[predicate $menuItem$]\label{def:menuItem}
Predicate $menuItem(x,y)$ denotes that place $x$ has a menu item $y$ on its 
menu.Formally it is defined as:
\begin{equation}\label{as:pred_menuItem}
\begin{gathered}
    is(menuItem,Predicate) \land \\
    arity(menuItem,2) \land \\
	argIs(menuItem,1,Restaurant) \land\\
	argClass(menuItem,2,FoodOrDrink)
\end{gathered}
\end{equation}
\end{definition}
As we see defined above (assertion \ref{as:pred_menuItem}), this predicate
constraints allow us to only use instances of class $Restaurant$ for the first
argument, and sub-classes of class $FoodOrDrink$ for the second. Example of this
is the following assertion that we use to have an example restauratn instance
in our KB (to continue the definition of Assertion \ref{as:restaurant1}), saying
that Restaurant1 has coffee on the menu:
\begin{equation}
menuItem(Restaurant1,Coffee)
\end{equation}

Besides the 
items on the menu, to explain our examples, we also need a way to tell that
a place provides some services.

\begin{definition}[predicate $providesServiceType$]\label{def:serviceType}
Predicate $providesServiceType(x,y)$ denotes that place $x$ provides service 
$y$. Formally it is defined as:
\begin{equation}\label{as:providesServiceType}
\begin{gathered}
    is(providesServiceType,Predicate) \land \\
	argIs(providesServiceType,1,Place) \land\\
	argClass(providesServiceType,2, Service)
\end{gathered}
\end{equation}
\end{definition}
As defined in the Assertion \ref{as:providesServiceType}, $providesServiceType$
constraints allow us to only use instances of class $Place$ (Public,Private,
Home,Restaurant for the current stage of the KB) for the first argument, and 
sub-classes of class $Service$ for the second. 

\begin{definition}[predicate $userLocation$]\label{def:userLocation}
Predicate $userLocation(x,y)$ denotes that user $x$ is located at place 
$y$. Formally it is defined as:
\begin{equation}\label{as:userLocation}
\begin{gathered}
    is(userLocation,Predicate) \land \\
	argIs(userLocation,1,User) \land\\
	argIs(userLocation,2, Place)
\end{gathered}
\end{equation}
\end{definition}
As defined in the Assertion \ref{as:userLocation}, $userLocation$
constraints allow us to only use instances of class $User$ for the first
argument, and instances of $Place$ as a second.

\begin{figure}[H]
	\centering
		\includegraphics[width=1\textwidth]{figures/fullExistingKB.png}
	\caption{Current "existing knowledge" on top of upper ontology}
	\label{fig:existingKB}
\end{figure}

At this point we have a formal KB structure representing a minimal 
\emph{existing knowledge} required to explan the examples from 
\autoref{tab:conversation1} and \autoref{tab:conversation2}. The KB is
graphically presented on the \autoref{fig:existingKB}, where the upper ontology
is presented in lighter color and new KB parts in darker. Due to lack of space,
the only relations are of $is$ (represented by blue arrows), $subclass$ 
(represented with black arrows) and $menuItem$ (orange arrow) predicates.

%SUBSECTION
\subsection{KA Knowledge}
\label{section:kakb}
In the previous two sections we defined the upper ontology
(Section \ref{section:upperOnto}) and then using its vocabulary to define the 
pre-existing knowledge (Section \ref{section:existingKB}). This will suffice to
support the explanation of the proposed KA approach. Similar as with the other
parts of the KB, listing full set of 
\emph{Curious Cat} KA rules would not fit in the paper. Instead we define an 
example set of the KB, sufficient for describing the approach and keeping the 
explanation as simple as possible. 

As a starting point, we need to define a main KA meta-class $Formula$, whichs 
is a special class (like $Number$).
\begin{definition}[class $Formula$ and its instances]\label{def:formula}
All logical formulas (assertions, queries) are instances of the class $Formula$.
Formally, this can be represented as:
\begin{equation}\label{as:formulas}
\begin{gathered}
	\forall P\forall x_{1...n}:is(P(x_1,...,x_n),Formula)
\end{gathered}
\end{equation}
Basically all of the content of the KB and queries are instances of $Formula$.
For example, assertion $userLocation(User1,Ljubljana)$ is an instance of 
$Formula$ and consequentially the statement $is(userLocation(User1,Ljubljana),
Formula)$ is true.
\end{definition}

Then we need to define additional KA meta predicates:

\begin{definition}[predicate "$known$"]\label{def:pred_known}
Special meta-predicate $known(x)$ denotes that the formula $x$ can be proven 
in the KB. This predicate is non-assertible, meaning that it cannot be used
to add things into the KB, but it can be used in inference rules. For example,
since the Assertion \ref{as:is_something} is already asserted in the KB and 
true, the query/un-assertible statement $known(is(Something,Class))$ is also 
true. The predicate is formally defined as follows:
\begin{equation}\label{as:known}
\begin{gathered}
	is(known,Predicate) \land arity(known,1) \land argIs(known,1,Formula)
\end{gathered}
\end{equation}
\end{definition}

In a similar fashion as predicate $known$, we define $unknown$ predicate.

\begin{definition}[predicate "$unkknown$"]\label{def:pred_unknown}
Special meta-predicate $unknown(x)$ denotes that the formula $x$ can 
\textbf{not} be proven in the KB. This predicate is non-assertible, meaning that it cannot be used to add things into the KB, but it can be used in inference 
rules. Building on the example given in the Definition \ref{def:pred_known}
($known$ predicate), the query $unkown(is(Something,Class))$ is 
\textbf{not true}, since the assertion exist an is known. On the other hand,
$unknown(is(Human,Duck))$ is true, since there is no knowledge in the KB which
would support the encapsulated $is$ statement. The predicate is formally defined as follows:
\begin{equation}\label{as:unknown}
\begin{gathered}
	is(unknown,Predicate) \land arity(unknown,1) \land argIs(unknown,1,Formula)
\end{gathered}
\end{equation}
\end{definition}

And now one of the main KA predicates, which is used to provide the CC system
with the formulas which need to be converted to NL and presented to the user.

\begin{definition}[predicate "$ccWantsToAsk$"]\label{def:pred_ccWantsToAsk}
One of the main KA predicates written as $ccWantsToAsk(x,y)$, denotes that the 
\emph{Curious Cat} system wants to ask user $x$ a question represented by the
formula $y$. For example, assertion 
\begin{equation*}
ccWantsToAsk(User1, userLocation(User1,x))
\end{equation*}
tells CC system to ask user the question $userLocation(User1,x)$, which,
after it goes through logic to NL conversion (\autoref{section:logicNL}) is 
paraphrased as
"Where are we?", as hinted in the step \ref{step:where} in the 
\autoref{tab:conversation1}. The predicate is formally defined as follows:
\begin{equation}\label{as:ccWantsToAsk}
\begin{gathered}
	is(ccWantsToAsk,Predicate) \land arity(ccWantsToAsk,2) \land\\ 
	argIs(ccWantsToAsk,1,User) \land argIs(ccWantsToAsk,2,Formula)
\end{gathered}
\end{equation}
\end{definition}

Now, after we have supporting predicates, we can define an example of KA rule, 
KA rules can written specifically to enable the production of questions for a 
narrow context, or generally, covering broad scope of questions as the
following:

\begin{equation}\label{as:generalRule}
\begin{gathered}
\forall i_1 \forall c \forall i_2 \exists s_2 \forall u:is(i_1,c) \land P(i_1,s) \land is(i_2,c)\land unknown(P(i_2,s_2)) \\ 
	\implies \\
ccWantsToAsk(u,P(i_2,\$x))
\end{gathered}
\end{equation}
This rather complicated material implication causes generation of question 
intents whenever there is an instance of a class that was used in an arity 2 
predicate, and there is another instance of the same class which doesnt have 
any assertion using this predicate. When the antecedent of this rule is true,
then the consequent is a $ccWantsToAsk$ predicate representing an open
ended logical query (NL question) asking for the things that can fullfill the
predicate $P$ for instances of aforementioned class.

If we take our example KB (\autoref{fig:existingKB}) and imagine we assert
an additional instance of a $Restaurant$ (Restaurant2: "Joes Pizza", as was 
done in the example conversation in \autoref{tab:conversation1}, 
step \ref{step:kind}, the premises of the rule can get satisfied like this:
\begin{equation}\label{as:generalRuleantecedent}
\begin{gathered}
is(Restaurant1,Restaurant) \land menuItem(Restaurant1,Coffee)\land \\
	is(JoesPizza,Restaurant) \land unknown(\exists s_2:menuItem(JoesPizza,s_2))
\end{gathered}
\end{equation}
Because all of the above premises are true, including the $unknown$ part
(there is no support in the KB for the formula inside $unknown$ predicate - see 
Definition \autoref{def:pred_unknown}), this rule in our KB up to now produces the 
following consequent:
\begin{equation}\label{as:generalRuleConsequent}
\begin{gathered}
	ccWantsToAsk(User1,menuItem(JoesPizza,x))
\end{gathered}
\end{equation}
representing an intent of CC system to ask user the question 
$menuItem(JoesPizza,x)$, which converted to NL comes out as "What is on the menu
in Joe's Pizza", which matches the step \ref{step:menu} from the 
\autoref{tab:conversation1}. In the general KA rule example (Assertion 
\ref{as:generalRule}), we can notice that one of the variables ($\$x$) is not 
presented with standard variable naming ($x,y,z$), but is marked with a prefix 
$\$$, which is a marker for the inference engine to not treat this as a standard
variable and try to bind it to values inside KB. Instead variables marked with
$\$$, are treated as normal concepts until they are asserted into the KB or used
as in the KB as a query. 

The rule \ref{as:generalRule} effectively detects when there is an instance in 
the KB that doesnt have some kind of information that other instances have, and
then it causes the system to intend to ask about it, if and when it has a 
suitable opportunity (e.g. a suitable interaction context). The rule described,
is an example of the general rule that can produce the apparent curiosity of 
the system using nothing but the existing background or newly acquired 
knowledge, whatever that may be.

In very large knowledge bases, general rules like this can produce many 
questions, including, in some cases, many irrelevant ones. To mitigate this, the
proposed system must have additional rules that can suppress questions on some 
predicates, or for whole parts of the KB. Simples example for this is to 
introduce $omitPredicate(x)$ predicate, with a constraint 
$argIs(omitPredicate,Predicate)$, which can be then either used int rules like
\ref{as:generalRule} as one of the conjunction premises using $unknown$
predicate, or the resulting $ccWantsToAsk$ sentences can be filtered with
additional rules, or by \emph{procedural component}, removing questions
using \emph{omitted} predicates.

While we defined rule \ref{as:generalRule} in some detail here to show the 
possibilities of the approach, we will explain the rest of the system through 
simpler examples for easier understanding. For example, the narrower rule which
produces the same step \ref{step:menu} of example shown in 
\autoref{tab:conversation1} can also be defined as follows:

\begin{equation}\label{as:specificRule}
\begin{gathered}
\forall x \forall u:is(x,Restaurant) \implies ccWantsToAsk(u,menuItem(x,\$y))
\end{gathered}
\end{equation}
In this case, the system would always produce the $menuItem$ questions for all, 
existing restaurants, regardless of whether this knowledge exists already or
not, it would always aks more. But only for $menuItems$, as the rule is not
general and would never come out of new type of questions as does rule 
\ref{as:generalRule}.

We can see now that the KA or "curiosity rules" can span from very general, to 
very specific. General rules can automatically trigger on almost any newly 
asserted knowledge, while specific ones can be added to fine-control the 
responses and knowledge we want to acquire. How specific or general the rule 
will be, is simply controlled by the number and content of the rule premises.

% SUBCHAPTER
\section{Context}
\label{section:context}
As mentioned in \autoref{section:bg:context}, in order to be able to ask 
relevant questions which users can actually answer, and at the same time 
maintain their interest, the context of the user is of crucial importance. For 
this reason, a considerable part of our KB content is user context, which can 
be used with the KA rules as the rest of the knowledge. One example of 
contextual knowledge is the $userLocation(x,y)$ predicate 
(Definition \ref{def:userLocation}), which holds the information combined from 
mobile 
sensors mining (see the definition of $probableUserLocation$ below) and the KA 
process. Besides $userLocation$, there is a lot of additional information on the
user that the proposed system uses to coming up with personalized questions.

\subsection{Mined Context}
\label{section:minedContext}
For the system to be able to address users at the right time and about the right
knowledge, it makes sense to understand user and his/her current context as
much as possible. While there is infinite amount of things that can contribute
to contextual knowledge, the central and most important piece in our 
approach is user's location and the duration of stay at this location. 

While for this (CC) approach it doesn't matter how exactly the current location
and other contextual raw data is acquired, since the year 2007 (release of 
iPhone and Nokia N95) it is the easiest to acquire it from the user's mobile 
phones\footnote{With the prior user's consent.}. As we hinted already in the
\autoref{section:SPD1}, we use a modified \emph{staypoint detection} algorithm, 
similar to the approach taken by \textcite{Kang2005} (\autoref{alg:spd1}). 
Here we will describe the mining 
algorithm only briefly, to be able to explain the approach. It is described in
more detail in section \ref{section:locationServlet}. These algorithms are all
implemented in our \emph{procedural component} 
(see\autoref{section:Architecture}, depicted on \autoref{fig:Architecture}).

This algorithm takes raw GPS readings as input, and clusters them into visits
(staypoints) and paths (moves between the staypoints), based on two thresholds:
the time ($T_t$) the user needs to stay
inside the perimeter ($T_P$), which is the second threshold. The staypoint is 
defined with the coordinates (of the users mobile device) that all lay inside a
given perimeter for at least minimum time: 
$r(lat,lon)<T_p \land t(lat,lon)>T_t$. The result of this algorithm is one
coordinate (lat, lon), time of arrival and the duration of stay, which can be
directly used by our approach (see \autoref{pred:probableUserLocation} below). 
This simple algorithm proves 
to be robust to the usual GPS signal, which is not always returning the same 
coordinates for the same location and is also lost during the time the user is
indoors, or nearby tall buildings or trees. 

After we have the coordinates, time of arrival and current duration of stay,
we can use this info to get additional information about the place, most easily
from one of the online APIs (Foursquare, Factual places, Google Places). This
usually return multiple results, where the less likely ones need to be filtered
out. This is described in detail in the implementation 
(section \ref{section:locationServlet}), 
related work (Section \ref{section:MarcoMamei}) and also in the related papers 
\autocite{Mamei2010,Bradesko2015}.

Let's say the user moves from the previous location. This is detected by the
algorithm (move), which causes the \emph{Procedural Component} to remove 
previous $userLocation(x,y)$ assertion for this user, since his/her location is
not known anymore. Then the SPD algorithm (section 
\ref{section:locationServlet} is monitoring the user, and 
once it detects that he/she stopped at some location for a bit, it retrieves 
the most likely place from public APIs. After this, the enriched and relevant 
information can be added 
back to the KB. For this we need to define (add) a few more pieces of the KB.

\begin{definition}[predicate "$probableUserLocation$"]
\label{pred:probableUserLocation}

Predicate $\\probableUserLocation(x,y)$ denotes that the user $x$ is most likely
at the location $y$. It is "probable" because it is automatically inferred by 
the ML algorithm and we still want the user to confirm it.
Formally the predicate is defined as:
\begin{equation}\label{as:probableUserLoc}
\begin{gathered}
    is(probableUserLocation,Predicate) \land \\
	argIs(probableUserLocation,1,User) \land\\
	argIs(probableUserLocation,2, Place)
\end{gathered}
\end{equation}
\end{definition}

\begin{definition}[class "$Visit$" and its instances]\label{def:visit}
Class $Visit$ represent a concept of a visit. Each instance represent one
particular visit, which can be used in other assertion to add more data to it
or to link it to other concepts. For example see Assertion 
\ref{as:joesPizzaVisit}.
\end{definition}

\begin{definition}[predicate "$userVisit$"]
\label{pred:userVisit}
Predicate $\\userVisit(w,x,y,z)$ denotes that the user $w$ did a visit $x$
at the time $y$, where visit duration was $z$. This predicate is used to be 
able to link users to their visits with some additional data like time of
arrival (sa unix time), and the duration of stay (in seconds), which proved to 
be an important part of the context. Formally the predicate is defined as:
\begin{equation}\label{as:userVisit}
\begin{gathered}
    is(userVisit,Predicate) \land \\
	arity(userVisit,4) \land \\
	argIs(userVisit,1,User) \land\\
	argIs(userVisit,2, Visit) \land \\
	argIs(userVisit,3,Number) \land \\
	argIs(userVisit,4,Number) \land
\end{gathered}
\end{equation}
\end{definition}

\begin{definition}[predicate "$placeVisit$"]\label{pred:placeVisit}
Predicate $\\placeVisit(x,y)$ denotes that the place $x$ was visited as part of
the visit $y$. This predicate is used to link visits to particular places.
Formally the predicate is defined as:
\begin{equation}\label{as:placeVisit}
\begin{gathered}
    is(placeVisit,Predicate) \land \\
	arity(placeVisit,2) \land \\
	argIs(placeVisit,1,Place) \land\\
	argIs(placeVisit,2, Visit) \land \\
\end{gathered}
\end{equation}
\end{definition}

\begin{figure}[h]
	\centering
		\includegraphics[width=1\textwidth]{figures/contextOntology.png}
	\caption{Current KB with newly added "context support knowledge" marked 
with orange.}
	\label{fig:contextKB}
\end{figure}

Now, after we defined the relevant predicates (see \autoref{fig:contextKB}), we 
can continue building the example.
Let's say that hte most likely place returned by SPD
(section \ref{section:locationServlet}), is "Joe's Pizza"
(our usual example from \autoref{tab:conversation1}). The 
\emph{Procedural Component} creates a new concepts  $JoesPizza$ and 
$Visit1$ (it has number 1 since it is the first instance in the KB), and adds 
them into the KB:
\begin{equation}\label{as:joesPizzaVisit}
\begin{gathered}
	is(JoesPizza,Place) \land is(Visit1,Visit)
\end{gathered}
\end{equation}
After this, the system adds more contextual data to these concepts. Let's say
the user arrived to the place on 19. Sept 2017 at 3pm, and is at the moment
there 1minute. This converts into the following assertions being added, while
removing possible prior assertions for the same user and visit combination:
\begin{equation}\label{as:probableContext}
\begin{gathered}
	probableUserLocation(User1,JoesPizza)
\end{gathered}
\end{equation}
and
\begin{equation}\label{as:visitContext}
\begin{gathered}
	userVisit(User1,Visit1,1505746800,60)
\end{gathered}
\end{equation}
Then, continuing our example, whenrver te system gets a new GPS coordinate
(let's say in 10 seconds), and consequently our algorithms calculate context 
update, the system deletes the Assertion \ref{as:visitContext}, and updates it
with new data:
\begin{equation}\label{as:visitContext1}
\begin{gathered}
	userVisit(User1,Visit1,1505746857,70)
\end{gathered}
\end{equation}
These updates (even if just duration of visit seconds) are causing the CC 
system inference engine to re-evaluate the rules connected to relevant 
predicates and thus possibly produce new consequents in the shape of new 
questions or comments, which appears to the users as proactivity, since they 
get new relevant questions even if not interacting with the system.
This simple contextual knowledge(presented with Assertions 
\ref{as:probableContext} and \ref{as:visitContext1}) is enough to be able to 
show how to make the system produce the examples provided in 
\autoref{tab:conversation1} and \autoref{tab:conversation2}.

At this moment careful reader might notice that the predicate $placeVisit$ was
not used in the assertions. The predicate is not yet asserted, because at 
this time it is not yet known for sure, whether the user is exactly at this 
place. The $probableUserLocation$ is only used to trigger questions for the
predicate $userLocation$ (see definition \ref{def:userLocation}). Only after 
this is answered, we can be
100\% sure that this is the exact place, and, besides the $userLocation$ (see
assertion \ref{as:joesPizzaVisit}), the system can also assert

\begin{equation}\label{as:placeVisit}
\begin{gathered}
	placeVisit(JoesPizza,Visit1)
\end{gathered}
\end{equation}
and thus permanently link the user's 19. Sept 2017 visit to the place 
Joe's Pizza.

%subsection
\subsection{Acquired context}
\label{section:acquiredContext}
Besides the context mined automatically, the system can also use internal 
context, which is a specific set of KA rules and KB knowledge acquired from
the user himself, or other users, and is directly relevant to the the him/her. 
By considering the knowledge acquired from the user, we improve relevance of 
asked questions. 
This knowledge is obtained by asking the user specific questions about himself,
such as, the languages spoken, profession, interests, preferred food, etc.,
represented by the example predicates below:
\begin{equation*}
\begin{gathered}
	userAge(x,y) \\	
	userSpeaksLanguage(x,y) \\
	userInterest(x,y)
\end{gathered}
\end{equation*}
or, a lot of knowledge can also be inferred already even by questions asked
in our examples (Table \ref{tab:conversation1}), resulting in many instances of
$Visit$ ($Visit1,Visit2,Visit3,...$) and $orderedFood$ predicates.
Specific rules or analytics provided by the \emph{Procedural Component} can
infer things like the following examples:
\begin{equation*}
\begin{gathered}
	userHome(x,y) \\
	userHomeCity(x,y) \\
	userHomeCountry(x,y) \\
	userLikesFood(x,y) \\
	...
\end{gathered}
\end{equation*}

The knowledge gathered in this way, is additional to the mined context and can 
be used by the rules to better identify the users who will actually be able to 
answer particular questions.

%section
\section{NL to logic and logic to NL conversion}
\label{section:nl}
In order to interact with the user, it is not sufficient to form the knowledge 
acquisition questions in logical form using an inference engine and KA rules,
as we had seen up to now. These formulas are understandable by knowledge 
engineering and math experts, but are not at all appropriate for a direct use 
in general KA using crowdsourcing from the general population. For this reason,
the logical formulas of the sentences and questions need to be translated to 
natural language to be presented to the user. Similarly, as the user is 
providing some of the answers in natural language these at least to some extent
have to be transformed from natural language, into their logical form.
This means that in addition to the knowledge itself the KB should include 
natural language description of the knowledge units, or the natural language 
generation capabilities must be provided by an external service (the letter is 
the case in our example with Umko KB \parencite{Bradesko2015} and
early corwdsourcing approach \parencite{Bradesko2012a}. 

Because natural language generation and conversion is not the main focus of 
this paper, we present here only a simplified version which explains the basic 
concepts involved. The actual Curious Cat implementation is based on 
Cyc NL\parencite{Baxter2005} for generation, and SCG\parencite{Schneider2015}
for the NL to logic. It consists of more than 90 predicates and rules beyond 
those in the baseline Cyc system to handle language generation. This is 
described in more detail in the Implementation chapter 
(Chapter \ref{chapter:implementation}, section \ref{section:locationServlet}).

%subsubsection
\subsection{Logic to NL}
\label{section:logicNL}
When the system already employs an extensible KB, each of the concepts in the 
KB can be named using a standard textual string, and predicates can have 
attached a knowledge on how to represent themselves in natural language. For 
this, we first need to add a concept of $String$ on top of our 
\emph{upper ontology}.

\begin{definition}[class $String$ and its instances]\label{def:string}
All written texts (strings), marked with double quotes, are instances of the 
class $String$. Formally, this can be
asserted into a KB as:
\begin{equation}\label{as:strings}
\begin{gathered}
	\mathbb{S}=\{x|\forall y:x="y"\} \\
	\forall x \in \mathbb{S}:is(x,String)
\end{gathered}
\end{equation}
\end{definition}
For example, the following is a correct logical statement and is also true:
\begin{equation*}
	is(\text{"This is string"},String)
\end{equation*}

Now we can also define the $name$ and $namePlural$ predicates, which are used 
to add to concepts their name string representations.

\begin{definition}[predicate "$name$"]\label{def:predName}
Predicate $name(x,y)$ denotes that the concept $x$ has a name, or string
representaion given as $y$. For example, asserting 
\\$name(JoesPizza,\text{"Joe's Pizza"})$, states that the string representation
for the concept $JoesPizza$ is "Joe's Pizza". Formally the predicate is 
defined as:
\begin{equation}\label{as:predName}
\begin{gathered}
	is(name,Predicate)\land\\
	arity(name,2)\land\\
	argIs(name,1,Something)\land\\
	argIs(name,2,String)
\end{gathered}
\end{equation}
\end{definition}

\begin{definition}[predicate "$namePlural$"]\label{def:predNamePlural}
Predicate $namePlural(x,y)$ denotes that the concept $x$ has a plural version of
its name representaion given as a string $y$. For example, asserting 
\\$namePlural(Pizza,\text{"pizzas"})$, states that the plural version of 
string representation for the concept $Pizza$ is "pizzas". Formally the 
predicate is defined as:
\begin{equation}\label{as:predNamePlural}
\begin{gathered}
	is(namePlural,Predicate)\land\\
	arity(namePlural,2)\land\\
	argIs(namePlural,1,Something)\land\\
	argIs(namePlural,2,String)
\end{gathered}
\end{equation}
\end{definition}

And similarly, $nlPattern$ predicate, attaching a NL generation information to 
other predicates:

\begin{definition}[predicate "$nlPattern$"]\label{def:predNlPattern}
Predicate $nlPattern(x,y)$ denotes that the predicate $x$ has a NL pattern
that can be used in sentences given as $y$. For example, asserting 
$nlPattern(is,\text{"\$1 is a \$2"})$, states that the predicate is, can
be represented in natural language as the name of first argument, followed by
the string "is a", and then followed by the name of the second argument. Based
on our example NL engine, the arguments are marked with the "\$" sign, followed
by the number. When some of the arguments are missing, this represents an 
interrogative mode. For example $nlPattern(is,\text{"What is \$1"})$. See below
for more details. Formally the predicate is defined as:
\begin{equation}\label{as:predNL}
\begin{gathered}
	is(nlPattern,Predicate)\land\\
	arity(nlPattern,2)\land\\
	argIs(nlPattern,1,Predicate)\land\\
	argIs(nlPattern,2,String)
\end{gathered}
\end{equation}
\end{definition}

To be able to explain the simple Logic to NL engine, let's add the following 
assertions to our KB:
\begin{equation}\label{as:menuitem1}
\begin{gathered}
	name(Coffee,\text{"coffee"})\land\\
	name(Restaurant1, \text{"L'Ardoise"})\\
	nlPattern(menuItem, \text{"\$1 has \$2 on the menu"})
\end{gathered}
\end{equation}

Now, with this NL knowledge, our NL generation engine, when represented by
the sentence 
\begin{equation}
\label{nl:menuItem1}
menuItem(Restaurant1,Coffee)
\end{equation}
checks into the KB, whether it 
has enough of NL knowledge to do it. First it tries to find all the patterns
of the predicate by issuing a query:
\begin{equation}\label{q:nlpattern1}
Q:nlPattern(menuItem,x)
\end{equation}
which results in all the possible values of $x$:
\begin{table}[H]
\centering
\caption{Results of query \ref{q:nlpattern1}.}
\label{tab:results1}
\begin{tabular}{|c|}
	\hline
	\textbf{x} \\
    \hline
    "\$1 has \$2 on the menu" \\
    \hline
\end{tabular}
\end{table}
Then if does similar search queries for both of the concepts in the arguments
of the predicate:
\begin{equation}\label{q:names1}
\begin{gathered}
Q:name(Restaurant1,x_1) \lor namePlural(Restaurant1,y_1)\\
name(Coffee,x_2) \lor namePlural(Coffee,y_2)\\
\end{gathered}
\end{equation}
which results in
\begin{table}[H]
\centering
\caption{Results of query \ref{q:names1}.}
\label{tab:results2}
\begin{tabular}{|c|c|c|c|}
	\hline
	\textbf{$x_1$} & \textbf{$y_1$} & \textbf{$x_2$} & \textbf{$y_2$}\\
    \hline
    "L'Ardoise" & & "coffee" & \\
    \hline
\end{tabular}
\end{table}
As we see in the result tables \ref{tab:results1} and \ref{tab:results2}, it
os able to find the pattern and also names of both of the arguments 
($\$1=Restaurant1$ and $\$2=Coffee$). These results are merged into the 
NL representation "L'Ardoise has coffee on the menu.". 
The engine checks the \emph{arity} of the $menuItem$ predicate, ,
and verifies whether all of the slots are bound to non variable concepts. When
this is fullfilled like in the example above, the engine knows that this 
sentence must be declerative (up to now we only have one declerative pattern), 
picks the proper pattern, and also adds the punctuation "." on it's own.

Now, consider, our Logic to NL engine gets the task to convert the formula
(query)
\begin{equation}
menuItem(Restaurant1,x)
\end{equation}
where $x$ represnets a variable as in other our 
examples. Our engine again, does the same query \ref{q:nlpattern1}, but because
the second argument is now a variable, only the first part ($Restaurant$) of 
the second query \ref{q:names1}. Results for $x_1$ are the same as before,
but there is no results for $x_2$. Now the engine detects that one of the 
pattern arguments ($\$2$) is missing, and since it does't have any additional
info on how to convert a statement into an interrogative mode, it simply
converts it into a KA pattern: "L'Ardoise has \_\_\_ on the menu.".

While these type of patterns is enough to be able to do the simple KA, it
doesn't yet completely follow the exaples from \autoref{tab:conversation1}. To
be able to do properly formed interrogative sentences when necessary, we need
to add additional assertion, similar as the last part of assertion 
\ref{as:menuitem1}.

\begin{equation}\label{as:menuitem2}
\begin{gathered}
	nlPattern(menuItem, \text{"What is on the menu in \$1"})
\end{gathered}
\end{equation}
Now the the query \ref{q:nlpattern1}, if issued again will return more
results:
\begin{table}[H]
\centering
\caption{Results of query \ref{q:nlpattern1}, after we have more $nlPattern$ 
assertions for the predicate $menuItem$.}
\label{tab:results3}
\begin{tabular}{|c|}
	\hline
	\textbf{x} \\
    \hline
    "\$1 has \$2 on the menu" \\
    \hline
	"What is on the menu in \$1" \\
	\hline
\end{tabular}
\end{table}
Now, considering that we are still converting $menuItem(Restaurant1,x)$, the
engine will among two options pick a second one, because it has a better match
against given arguments (only argument 1 is present), and thus instead of
"L'Ardoise has \_\_\_ on the menu." convert into 
"What is on the menu in L'Ardoise?"

Careful reader might notice that the results for plural versions of concept 
names ($namePlural$) were always empty. For the concept $Coffee$, the plural
version is exactly the same as for singular, so we omit it. In cases when
this is not so, the engine picks the plural version when it is describing
instances of $Class$, representing whole classes of terms which can have 
multiple instances, and singular version when describing instances of
other terms.

\hl{add other required definitions to be able to do full example}

The definitions (\autoref{def:predNlPattern}, \autoref{def:predName} and
\autoref{def:predNamePlural}),
are simplifified formulations of minimal required NL knowledge to be able to 
present our approach. While the implementation of NL generation engine is 
totally up to the developer of such a system, for this examples (and also 
our implementation - see section \ref{section:cycnl}), we decided to 
put the NL patterns
into the KB, next to the definitions of the concepts. On the contrary to
having the NL patterns in some external database or a text file, this approach
in long term allows the system to ask questions about the language patterns
themselves, and thus allow it to learn and ask about these as well. For example,
if instead of strings directly, NL patterns would be constructed from the 
concepts of words (not part of our KB and not consistant with it, but serves 
as an hypotetical exaple, and more tightly matches our non-simplified 
implementation):
\begin{equation*}
\begin{gathered}
is(WordForPizza, Word) \land name(WordForPizza, "pizza")\land\\
nameWord(Pizza,WordForPizza)
\end{gathered}
\end{equation*}
then the system could notice that its missing the plural version of the word
for the concept $Pizza$ and ask for it: $namePlural(WordForPizza,x)$ 
(converted to NL: "What is the plural word for 'pizza'?".

\begin{figure}[H]
	\centering
		\includegraphics[width=1\textwidth]{figures/LogicToNLOntology.png}
	\caption{Current "NL generation knowledge" on top of the existing KB (new
    additions mareked with blue)}
	\label{fig:nlKB}
\end{figure}

%subsubsection
\subsection{NL to Logic}
\label{section:NLLogic}
While simple processing based on the same NL knowledge as entered and described
in the previous section (\autoref{section:logicNL}) suffices to interpret 
isolated terms and denotational phrases from the user responses, converting 
general NL expressions back to logic is much trickier than the other way 
around, because natural language is much more ambiguous than logic. This 
process exceeds the scope of this KA related thesis and is 
described in more detail in the implementation (section \ref{section:cycnl}), 
and paper 
\parencite{Schneider2015} and to other aspects of it also in chat-bot related
works\parencite{Wallace2013,Wilcox2011}. Following the simplistic examples as
before just to show and be able to explain the approach, we explain the basic 
functionality (enough to be able to make our examples from 
\autoref{tab:conversation1}).

In the proposed NL engine (described also in section \ref{section:logicNL}
above), all of the \emph{String} arguments of the predicates $name$, 
$namePlural$ and $nlPattern$ are indexed and searchable by the system, where
the $\$\#$ slots of the $nlPattern$ strings are replaced with asterisks ($*$),
representing any number of words, in a similar way as chat-bot scripts are
matching defining their patterns\parencite{Wilcox2011}.

For illustration consider inverting the example \ref{nl:menuItem1} from the
\autoref{section:logicNL}. The system is presented with the statement
\begin{equation}\label{nl:joespizzamenu}
\text{"Joe's Pizza has coffee on the menu."}
\end{equation}

Because all the textual representations of the concepts are indexed, the NL
engine is able to search for the strings appearing in the sentece through
our NL predicate assertions and finds the following concepts:
\begin{table}[H]
\centering
\caption{String search results for query \ref{nl:joespizzamenu}.}
\label{tab:nlResults1}
\begin{tabular}{|l|l|}
	\hline
	\textbf{String} & \textbf{Concept} \\
    \hline
    "Joe's Pizza" & $Restaurant1$ \\
    \hline
    "* has * on the menu" & $menuItem$ \\
    \hline
    "coffee" & $Coffee$ \\
    \hline
\end{tabular}
\end{table}

Because of the $is$ and $subclass$ knowledge of $Restaurant1$ and $Coffee$ 
classes, which can be observed in \autoref{fig:nlKB}, and the $argIs$ and 
$argClass$ argument constraints of $menuItem$ (Definition \ref{def:menuItem}), 
the inference engine is actually able to use the results above 
(\autoref{tab:nlResults1}), to construct the logical equivalent of the above 
sentence: 
\begin{equation*}
menuItem(Restaurant1, Pizza)
\end{equation*}
This is done in the following sequence:
\begin{enumerate}
\item The system first finds argument constraints of all the predicates that are
found in the results. In this case it is $argIs(menuItem,1,Restaurant)$ and
$argClass(menuItem,\\
2,FoodOrDrink)$ for the predicate $menuItem$.
\item All of the predicates are checked for their arity, and removed if the
arity of the predicate is not the same as the number of concepts that were found
as part of the sentence. In this case the arity is 2 (Assertion 
\ref{as:pred_menuItem}), and we found 2 unique concepts ($Coffee$ and 
$Restaurant1$).
\item The concepts that are not filtered out in the previous steps are then 
checket whether they fullill the the arity constraints of the arguments of the
predicates. In our above example, $Coffee$ is a subclass of $FoodOrDrink$, so
it can be used as argument 2, and $Restaurant1$ is an instance of $Restaurant$,
so it can be used as argument 1.
\end{enumerate}

To make the example more complicated, and thus process more clear, consider
an additional predicate $servesCuisine$ which has a totally different meaning
than $menuItem$, but can have the same language pattern:
\begin{equation*}
\begin{gathered}
    is(servesCuisine,Predicate) \land \\
	arity(servesCuisine,2)\land\\
	argIs(servesCuisine,1,Restaurant) \land\\
	argClass(servesCuisine,2, Cuisine) \land \\
	nlPattern(servesCuisine, \text{"\$1 serves \$2"}) 
\end{gathered}
\end{equation*}
while we add one more pattern to the $menuItem$ predicate.
\begin{equation*}
\begin{gathered}
	nlPattern(menuItem, \text{"\$1 serves \$2"}) 
\end{gathered}
\end{equation*}
With this additional predicate and NL knowledge, the engine, given the following
text:
\begin{equation}\label{nl:joespizzamenu1}
\text{"Joe's Pizza serves coffee."}
\end{equation}
will find the following possible terms:

\begin{table}[H]
\centering
\caption{String search results for query \ref{nl:joespizzamenu1}.}
\label{tab:nlResults2}
\begin{tabular}{|l|l|}
	\hline
	\textbf{String} & \textbf{Concept} \\
    \hline
    "Joe's Pizza" & $Restaurant1$ \\
    \hline
    "* serves *" & $menuItem$ \\
    \hline
	"* serves *" & $servesCuisine$\\
	\hline
    "coffee" & $Coffee$ \\
    \hline
\end{tabular}
\end{table}
Now on the \autoref{tab:nlResults2} we can see, that there are two predicates
for the same NL pattern. But when the system will try to fit the class $Coffee$
into both of them, it will not fit to $servesCuisine$, because it requires
subclass of $Cuisine$, but $Coffee$ is a sub-class of $Drink$. Consequently,
the predicate $servesCuisine$ will be filtered out, and the system will
properly disambiguate into the logical equivalent
\begin{equation*}
	menuItem(Restaurant1,Coffee)
\end{equation*}

These two examples illustrate a simplest kind of sentential NL conversion. 
When there are many more possible concepts which need to be combined, the 
complexity of the problem quickly explodes, or is ambiguous even given the 
constraints. For this reason, our actual Cyc SCG implementation contains much 
more complex KB structures and patterns which help with the conversion and a 
high-speed parser.

For the most complicated NL conversions, similar approach to a standard 
ChatScript or AIML patterns\parencite{Wilcox2011,Wallace2013} is taken, just 
that the patterns are not matched to textual response patterns directly, but 
to the logical statements instead. Consider the example answer given as step 
\ref{step:menu} in \autocite{tab:conversation1}, when user would, instead of 
simple "pizzas", answer something like "They sell pizzas", or "They have 
pizzas on the menu". In this case the simple approach described at the 
beginning of this chapter would not find anything and thus the system needs 
more descriptive patterns like
\begin{equation}\label{nl:complexPattern}
\begin{gathered}
nlPattern(menuItem,\text{"\$1 (sell|sells|has|have) \$2 |(on the menu)"})
\end{gathered}
\end{equation}
where the sign $|$ represent the or clause, meaning that the "on the menu" is
optional and in the middle, only one of the words ("sell","sells",...) will
be used. Additionally, the system needs to infer that "They" refers to the
restaurant where user currently is (first argument), and that "Pizzas" refer 
to the the second argument.
Additional help that the NL system has and we didn't mention is the predicate
we used to generate a question. For example, in the case of step 
\ref{step:menu}, the system can give advantage to $menuItem$ predicate and its
argument constraints, even if there are multiple ambiguous predicates or other
terms matched by the strings.

Also, note that these more complicated patterns are to be used only when the 
assisted KA doesnt present valid options and user types free text and also, that
the patterns dont ever specify what should be the systems response (as opposed
to how chatbots work). Instead the pattern just converts the text to the logic
and then leaves up to the inference engine what to do next. This is completely 
different approach from standard chatbot systems where patterns dictate the 
responses as well. For readers interested in more detailed overwiew of chat-bot
approaches and patterns please refer to chapter Related Work (subchapters
\ref{section:r:aiml} and \ref{section:chatscript}), or related paper
\parencite{Bradesko2012}.

%subsubsection
\subsection{Dialog Formulation}
\label{section:dialog}
As mentioned before, \emph{Curios Cat} conversation is not pre-defined by the 
patterns, but is completely knowledge driven. The knowledge is being inserted
into the system by its users (section \ref{section:consistency}), and automated
context (section \ref{section:context}). This is then picked up by the 
inference engine that generates questions and commens with the help of KA rules
(section \ref{section:kakb}). The resulting assertions, which propose the 
intents like $ccWantsToAsk(x,y)$ (definition \ref{def:pred_ccWantsToAsk}) and
$ccWantsToComment(x,y)$ (definition \ref{def:ccWantsToComment} below), 
are then converted to the natural language and shown to the users, which then 
respond. The appearance of the assertions and consequently questions/comments
in NL and their user responses form a conversation. The topic and flow of the 
conversation in the most simplistic form depends on the order that these
assertions appear in the KB (the newest ones are always displayed first).

To be able to produce commenting or advice statements, beside the questions
(as shown also in the examples in \autoref{tab:conversation1}), we are missing
one more predicate.

\begin{definition}[predicate "$ccWantsToComment$"]
\label{def:pred_ccWantsToComment}
This conversational predicate written as $ccWantsToComment(x,y)$, denotes that 
the \emph{Curious Cat} system wants to tell (or comment) a statement $y$ to
the user $x$. For example, assertion 
\begin{equation*}
ccWantsToComment(User1, menuItem(Restaurant1,Coffee)
\end{equation*}
tells CC system to tell user $menuItem(Restaurant1,Coffee)$, which,
after it goes through logic to NL conversion (section \ref{section:logicNL}) 
is paraphrased as "L'Ardoise has coffee on the menu.". The predicate is 
formally defined as follows:
\begin{equation}\label{as:ccWantsToComment}
\begin{gathered}
	is(ccWantsToComment,Predicate) \land \\
	arity(ccWantsToComment,2) \land\\ 
	argIs(ccWantsToComment,1,User) \land \\
	argIs(ccWantsToComment,2,Formula)
\end{gathered}
\end{equation}
\end{definition}

For the experiments produced in this paper, \emph{Curious Cat} system 
(Procedural Component) tracks additional context of users current concept 
(topic) of interest (it asserts it into KB with $currentTopic$ predicate), and 
also a list of newly created concepts by that user. For example, concepts 
$JoesPizza$ and $PizzaDeluxe$ from our conversation example are on this ordered
list. In cases it has multiple $ccWantsTo...$ formulas in place, it prefers 
the formulas which contain these concepts in the given order. Then it simply 
exploits the feature of the system to produce new questions/comments on the fly
as all users (including the one having the conversation) and contextual data 
are constantly producing new knowledge and consequently new questions/comments.
It then presents these questions/comments to the user as they come in. For 
example, when user answers that he is at Joe's Pizza restaurant, the system 
produces $ccWantsToAsk(User1, menuItem(JoesPizza,x))$, which is showed to the 
user in NL, as it is the newest formula that appeared and contains the concept 
of JoesPizza. This simple approach works quite well, since the latest formula 
is always a consequence of the previous user action. Additionally, this can be 
used for pro-activity, when the question/comment for the user appears as a 
consequence of some other user's action, or a context change. If the user didnt
use the system for a while, we can simply present him with the latest formula 
when it appears from the inference as a proactive question/comment. In the cases
when we have a statement, and a question formula, the system presents the 
comment and then a new question as part of the same response (as also visible
in example convesation in \autoref{tab:conversation1}).

For the cases when the system needs more explicit control over the order of 
the questions, the inference rules can be added using special intent predicates
which allow ordering of the responses. For example predicates like 
$ccWantsTo...$, but with arity of 3, where new argument is importance, and then
the system picks more important predicates first. Additionally, the system 
tracks the window of user and its own responses with logical assertions like 
$ccResponse(user,formula,num)$ and $userResponse(user,formula,num)$ which 
stores the history of the conversation and allows rules like:
\begin{equation*}
\begin{gathered}
	\forall x \forall y_1 \forall y_2: userResponse(x,y_1,0) \land userResponse(x,y_2,1) \land equals(y_1,y_2) \\
	\implies \\
	ccWantsToComment(x,userRepeatingItself(x))
\end{gathered}
\end{equation*}
This (purely) example rule will produce a comment that user is repeating itself,
when the user will respond with the same thing two times in a row.

At last, in the occasions when there are no new questions appearing on the list,
the system will pick from the old ones in the order, until something will 
trigger a new one, or until depleting all of them. If the depletion happens, 
then the topic will traverse all the concepts from the history and thus 
accidentally trigger additional inference with new topic of interest for the 
user, which will produce new questions, which will additionally (if answered) 
produce new follow-up questions/comments. As the final fallback, the system 
will randomly pick a concept from its KB and present it to the inference 
engine as a current interest for the user.

\hl{revise this section and possibly add missing assertions when constructing
how everythin fits togehter. Maybe this chapter is not even necessary?}

%Subsection
\section{Consistency Check and KB Placement}
\label{section:consistency}
As already described, we can employ the inference engine to deduce various 
facts using forward-chaining inference including intents to ask a question. 
The answers can then be automatically inserted into the KB. The knowledge 
stored in the KB can be then retrieved using logical queries, which return 
the matching knowledge, or it can infer additional entailed knowledge at 
query time using backward-chaining inference. The queries are simple logical 
formulas, which the inference tries to prove or satisfy. For example, the
query
\begin{equation}\label{q:food}
	Q:class(x,Food)
\end{equation}
will return the following results if queried over our current KB 
(\autoref{fig:nlKB})

\begin{table}[H]
\centering
\caption{Results for query \ref{q:food}.}
\label{tab:nlResultsFood}
\begin{tabular}{|c|}
	\hline
	\textbf{x} \\
    \hline
    $Bread$ \\
    \hline
    $Baguette$ \\
    \hline 
	$Pizza$ \\
	\hline
	$Margherita$ \\
	\hline
\end{tabular}
\end{table}
Or the query
\begin{equation}
is(Restaurant1,Place)
\end{equation}
will return $True$.

For ilustration, let us assme that the system finds the following question to
ask 
\begin{equation*}
\begin{gathered}
\text{"What has Joe's Pizza on the menu?"}\\
(menuItem(JoesPizza,x))
\end{gathered}
\end{equation*}
with the argumentconstraints on the predicate as defined in assertion 
\ref{as:pred_menuItem}. Assuming that the user answers with "Pizza Margherita"
(hypothetical example, different than in \autoref{tab:conversation1}, step
\ref{step:pizzadeluxe}) which exists in our KB, the system can ask the inference
engine the following type of general query:
\begin{equation}
\begin{gathered}
    Q:name(x,\$answer)\land \\ % TERM?=x
    argIs(\$pred,\$argPos,y)\land \\ %ARGIS?=y
    is(x,y)\land \\
    \left(
    \begin{tabular}{c}
        $argClass(\$pred,\$argPos,z) \land  subclass(x,z)$\\
        $\lor$\\
        $unknown(\exists z:argClass(\$pred,\$argPos,z))$ \\
    \end{tabular}
    \right)
\end{gathered}
\end{equation}
where we replace the meta variables (marked with $\$$) with our values from 
the exaple question, where $\$answer$ represents user answer, $\$pred$ 
represents the predicate that was used to propose a question, and 
$\$argPos$ is the position of the variable in the question query. After we
fill in our values ($\$answer=\text{"Pizza Margherita}, \$pred=menuItem, 
\$argPos=2$), we get the following concrete logical query:
\begin{equation}\label{q:constraintQuery}
\begin{gathered}
   Q:name(x,\text{"Pizza Margherita"})\land \\ 
    argIs(menuItem,2,y)\land \\
    is(x,y)\land \\
    \left(
    \begin{tabular}{c}
        $argClass(menuItem,2,z) \land  subclass(x,z)$\\
        $\lor$\\
        $unknown(\exists z: argClass(menuItem,2,z))$ \\
    \end{tabular}
    \right)
\end{gathered}
\end{equation}
If this query is asked in our current KB (\autoref{fig:nlKB}), we get the 
following results:

\begin{table}[H]
\centering
\caption{Results for query \ref{q:constraintQuery}.}
\label{tab:constraintResults}
\begin{tabular}{|c|c|c|}
	\hline
	\textbf{x} & \textbf{y} & \textbf{z} \\
    \hline
    $Margherita$ & $Class$ & $FoodOrDrink$ \\
    \hline
\end{tabular}
\end{table}
Because the inference engine was able to answer this query, we imediatelly
know that the answer is consistent with the KB and is safe to assert it as 
$menuItem(JoesPizza,Margherita)$. 
\hl{Maybe switchh from Joes to
Lardoise which is for sure in the KB. Check whether worth mentioning Nothing}.

At this point we have seen an example of how to add new knowledge when the
concept that the user has provided already exists in the KB and the answer is 
structurally valid. But what would happen if user should say "Pizza Deluxe" 
(as in our example in \autoref{tab:conversation1}, step 
\step{step:pizzadeluxe}), which we do not have in the KB. In this case the query
above would return nothing. This would happen as well, if the user should say 
"car" (as in the step \ref{step:car}). When the validation query does not return
results (as would happen for "car"), we need to separately check whether the 
concept exists:
\begin{equation}\label{q:nameCar}
Q:name(x,"car")
\end{equation}
If it does (i.e. the querly \ref{q:nameCar} above returns the resulting 
concepts), then the answer is actually invalid on structural grounds (the term 
it includes ca not be used a viable answer).
But, if the concept does not exist yet (nothing returned, as would be the case 
for "Pizza Deluxe"), then we can simply create it together with its NL 
denotation and assert it using our question predicate.

%subsection
\subsection{Detailed Placement in the KB}
\label{section:placement}
In addition to deciding to add some newly acquired knowledge to the KB, the 
exact location in the "KB graph" is determined by the $argIs$ and $argClass$
assertions on the question predicate. Consider our example question
$menuItem(JoesPizza,x)$. Because of the $argClass(menuItem,2,FoodOrDrink)$, the
answer must be a subclass of $FoodOrDrink$ concept. Let us say the aswer is
"Pizza Deluxe", as with previous explanations. To insert the knowledge, the 
system goes through the steps described in the previous section 
\ref{section:consistency}. Because of $argClass$ assertions $PizzaDeluxe$ 
concept is added as a $subclass$ of $FoodOrDrink$ as illustrated in 
\autoref{fig:kbPosition}.

\begin{figure}[H]
	\centering
		\includegraphics[width=1\textwidth]{figures/kbPosition.png}
	\caption{$FoodOrDrink$ part of the class hierarchy from our example KB with
    newly added concept.}
	\label{fig:kbPosition}
\end{figure}

While this is logically valid, it is not detailed enough to satisfy our KA 
requirements. For this reason, when the concept does not already exist, or is 
not detailed enough (too high in the class hierarchy), we can issue additional 
query:
\begin{equation}\label{q:additionalFood}
Q:subclass(x,FoodOrDrink)
\end{equation}
which, for our example knowledge it returns:
\begin{table}[H]
\centering
\caption{Results for query \ref{q:additionalFood}.}
\label{tab:foodResults}
\begin{tabular}{|c|}
	\hline
	\textbf{x} \\
    \hline
    $FoodOrDrink$ \\
    \hline
    $Drink$ \\
    \hline
    $Coffee$ \\
    \hline
    $Food$ \\
    \hline
    $Bread$ \\
    \hline
    $Baguette$ \\
    \hline
    $Pizza$ \\
    \hline
    $Margherita$ \\
    \hline
\end{tabular}
\end{table}
This gives us various options.
\begin{enumerate}
\item Ask the user, which one of these PizzaDeluxe is (excluding the main 
class $FoodOrDrink$). For example: "What describes it in most detail?", or: 
"Is Pizza Deluxe a drink, coffee, bread, baguette, pizza or margherita"
\item Ask for the first level subclasses first, then for the next level, 
etc.: 1. "Is Pizza Deluxe a type of drink or food?", 2. "Is Pizza Deluxe a 
type of pizza?"
\item (which is usually the best option) we can scan all of the resulting 
concepts (results from \autoref{tab:foodResults}) for their NL patterns and 
names, and then match the strings to "Pizza Deluxe", to see which one fits the 
best. In this case, only the $Pizza$ concept provides a partial match ("Pizza"
vs "Pizza Deluxe"). So, we can immediately ask: "Is Pizza Deluxe a type of 
Pizza?". If the user agrees, we can, in addition to 
$subclass(PizzaDeluxe,FoodOrDrink)$, assert: $subclass(PizzaDeluxe,Pizza)$. 
This results in our new concept being located in the KB at the most descriptive
place as illustrated in \autoref{fig:kbCorrectPosition}.
\end{enumerate}

\begin{figure}[H]
	\centering
		\includegraphics[width=1\textwidth]{figures/kbCorrectPosition.png}
	\caption{New position of the $PizzaDeluxe$ concept in our KB.}
	\label{fig:kbCorrectPosition}
\end{figure}

%section
\section{Crowdsourcing Mechanisms}
\label{section:crowdsourcing}
Up to this point, we have discussed the mechanisms of KA, showing how it is 
possible to get valid knowledge from a single user. In order to lower the cost 
of and/or increase the speed of KA we can involve a crowd. Crowdsourcing 
however bring several challenges (as described in Section 
\autoref{section:problemCrowd}) including the followinga:
\begin{itemize}
\item User privacy
\item Users making deliberately false claims or having mistaken ideas about the 
world
\item Fast changing state of the real world
\end{itemize}
We tackle this by organizing our KB into smaller, hierarchical knowledge base 
structures. Each of these structures is then our virtual knowledge base in 
which we operate, and which has its own independent knowledge, added on top of 
all the sub-KBs up in the hierarchy (\autoref{fig:mt}). In the Cyc system, these
contextual KB structures are called \emph{Microtheories}\parencite{Kleer2013}.

\begin{figure}[H]
	\centering
		\includegraphics[width=1\textwidth]{figures/microtheories.png}
	\caption{Hierarchical structure of knowledge bases.}
	\label{fig:mt}
\end{figure}

The idea is that the sub-KBs which are higher up in the hierarchy are not aware
of the knowledge that is at the lower levels. But the KBs which are at the lower
levels, contain all the knowledge of their ancestors from the higher levels. 
For example,  let us assume that $WholeWorldKB$  in \autoref{fig:mt} has the 
knowledge defined in section \ref{section:upperOnotlogy} and 
\autoref{section:existingKB}, $CuriousCatBelievesKB$ has the knowledge defined
in section \autoref{section:kaKnowledge} and then acquired from multiple users. 
$User1KB$ has knowledge that was acquired from $User1$ and $user2KB$ has 
knowledge acquired from $User2$ (new, other user). Following our KB structure 
given in \autoref{fig:existingKB} and formal logical definitions, 
we can see that:
\begin{itemize}
\item $WholeWorldKB$ contains the upper ontology and pre-existing knowledge 
from \autoref{fig:existingKB}.
\item $CuriousCatBelievesKB$ contains KA assertions and union of assertions 
acquired from all the users.
\item $User1KB$ contains assertions acquired from user 1 and only relevant to 
her/him.
\item $User2KB$ contains assertions acquired from user 2 and only relevant to 
her/him.
\end{itemize}

Following this structure, it becomes obvious that each user in our system has 
its own sub-KB, connected to the main knowledge only through 
$CuriousCatBelievesKB$. Also, $User1KB$ and $User2KB$ cannot see each others 
knowledge, but only perceive the world through the "eyes" of 
$CuriousCatBelievesKB$ and their own local sub-KB. This means, that if $User1$ 
lies about something, the wrong knowledge will be only available to $User1$, 
while the rest of the users will not be affected. In this way, the users privacy
is also protected.

To benefit from crowdsourcing we want to share some of the knowledge to all of 
the users. Since each sub-KB can "see" only the assertions stored in itself and 
the assertions higher up in the hierarchy, we can control what only one user 
knows, versus all the users, by moving the specific assertions up or down 
through the KB hierarchy. 
We are proposing two approaches: 
\begin{enumerate}
\item Crowdsourcing through repetition (\autoref{section:crowdRepetition}),
\item and 2) Crowdsourcing through voting (\autoref{section:crowdVoting}).
\end{enumerate}
Crowdsourcing 1 is more advanced, since it includes type 2 as well, once the 
repeated assertions from multiple users got promoted to $CuriousCatBelievesKB$. 
Because it only promotes the knowledge which is asserted by multiple users 
independently, there is reduced chance for temporal wrong knowledge staying in 
our system before the voting from approach 2 removes it. In this sense, 
Crowdsourcing through repetition gives better results, but it has a drawback, 
especially if there is not enough of users in the system, the knowledge takes 
longer to be promoted to the main KB where other users could benefit from it. 
Additionally, users might have a feeling that they are the only one in the 
system if they dont see some immediate feedback and activity. 
Initially we started with Crowdsourcing through repetition, but then 
decided (due to reasons above) to do our initial experiments 
(described in \autoref{chapter:evaluation}, \autoref{section:resultsvoting}) 
by using the "voting" option.

%subsection
\subsection{Crowdsourcing through repetition}
\label{section:crowdRepetition}
This type of proposed crowdsourcing mechanism is based on the number of 
identical assertions in all the sub-KBs on the same level (i.e., different 
users providing exactly the same knowledge). Once the specific assertion count 
is above a threshold, we assume that there is enough evidence for promoting it 
to general knowledge, which is performed by moving the knowledge to higher 
level of the hierarchy (via "lifting rules" in the $CuriousCatBelievesKB$), 
and thus making it visible for all the users. After the knowledge is in the 
public KB, crowd users can start voting on it (see section 
\ref{section:crowdVoting}).

Consider the case of $User1$ answering the question from the previous examples: 
"What did you order?", with a lie: "spicy unicorn wings". The system will go 
through steps described in \autoref{section:consistency}
(\nameref{section:consistency}), and if the user confirms this new 
concept, \emph{Curious Cat} will believe (in the world for $User1$), that he 
ate spicy unicorn wings and that the $JoesPizza$ restaurant has them on the
menu. The contextual KB $User1KB$ would then get the assertion 
$menuItem(JoesPizza,SpicyUnicornWings)$. There is a very low chance that any 
other user would provide the same answer, so the wrong assertion will never get 
promoted to higher levels of KB.

On the other hand, if $User1$ answers $menuItem(JoesPizza,PizzaDeluxe)$, then 
$User2$ answers the same, then $User3$, the assertion already has a count of 
more than 2, which is the threshold in our system, and the assertion will get 
promoted to be visible for all the users (world). For newcomers to $JoesPizza$, 
the system will already know that they have "Pizza Deluxe" on the menu.
The real world implementation example of promoted knowledge at the first 
visit can be seen on \autoref{fig:androidPublic}.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.5\textwidth]{figures/androidPublic.png}
	\caption{Screenshot of Curious Cat displaying an already public information about Joes Pizza to a visiting user.}
	\label{fig:androidPublic}
\end{figure}

%subsection
\subsection{Crowdsourcing through voting}
\label{section:crowdVoting}
Contrary to the previous example, we can also promote all the answers 
immediately. In this case, all the assertions except the private ones 
(e.g. ones that contain the concept for the $User\#$), will get asserted into 
both the users KB and the $CuriousCatBelievesKB$.

Once the assertions are in $CuriousCatBelievesKB$, users will not get the 
standard questions produced by the KA rules, but will still get occasionally 
the "crowdsourcing" questions (produced by different rules), which will 
simply be checking the truth of some existing knowledge: "Is it true that Joes 
Pizza has Pizza Deluxe on the menu?". These simple yes/no questions allow us to 
assess the truthfulness of the logical statement and remove it, if it gets 
more negative answers than positive.

This voting mechanism serves for promoting knowledge (as described in 
\autoref{section:crowdRepetition} (\nameref{section:crowdRepetition}) and for 
detecting when the world changes and something that was true in the past is not 
true anymore.

%section
\section{Putting it All Together}
\label{section:together}
This section wraps up the approach and brings together the components described
in \autoref{chapter:approach} into a coherent full system, which we describe 
through the example conversation presented in \autoref{tab:conversation1} and
\autoref{tab:conversation2}.
\hl{FINISH THIS}
